GitHub Repositories Data

Repository: mattermost
URL: https://github.com/kavspvt2803/mattermost
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Mattermost is an open source platform for secure collaboration across the entire software development lifecycle. This repo is the primary source for core development on the Mattermost platform; it's written in Go and React and runs as a single Linux binary with MySQL or PostgreSQL. A new compiled version is released under an MIT license every month on the 16th.
Deploy Mattermost on-premises, or try it for free in the cloud.

Learn more about the following use cases with Mattermost:

DevSecOps
Incident Resolution
IT Service Desk

Other useful resources:

Download and Install Mattermost - Install, setup, and configure your own Mattermost instance.
Product documentation - Learn how to run a Mattermost instance and take advantage of all the features.
Developer documentation - Contribute code to Mattermost or build an integration via APIs, Webhooks, slash commands, Apps, and plugins.

Table of contents

Install Mattermost
Native mobile and desktop apps
Get security bulletins
Get involved
Learn more
License
Get the latest news
Contributing

Install Mattermost

Download and Install Mattermost Self-Hosted - Deploy a Mattermost Self-hosted instance in minutes via Docker, Ubuntu, or tar.
Get started in the cloud to try Mattermost today.
Developer machine setup - Follow this guide if you want to write code for Mattermost.

Other install guides:

Deploy Mattermost on Docker
Mattermost Omnibus
Install Mattermost from Tar
Ubuntu 20.04 LTS
Kubernetes
Helm
Debian Buster
RHEL 8
More server install guides

Native mobile and desktop apps
In addition to the web interface, you can also download Mattermost clients for Android, iOS, Windows PC, macOS, and Linux.
    
Get security bulletins
Receive notifications of critical security updates. The sophistication of online attackers is perpetually increasing. If you're deploying Mattermost it's highly recommended you subscribe to the Mattermost Security Bulletin mailing list for updates on critical security releases.
Subscribe here
Get involved

Contribute to Mattermost
Find "Help Wanted" projects
Join Developer Discussion on a Mattermost server for contributors
Get Help With Mattermost

Learn more

API options - webhooks, slash commands, drivers, and web service
See who's using Mattermost
Browse over 700 Mattermost integrations

License
See the LICENSE file for license rights and limitations.
Get the latest news

X - Follow Mattermost on X, formerly Twitter.
Blog - Get the latest updates from the Mattermost blog.
Facebook - Follow Mattermost on Facebook.
LinkedIn - Follow Mattermost on LinkedIn.
Email - Subscribe to our newsletter (1 or 2 per month).
Mattermost - Join the ~contributors channel on the Mattermost Community Server.
IRC - Join the #matterbridge channel on Freenode (thanks to matterircd).
YouTube -  Subscribe to Mattermost.

Contributing

Please see CONTRIBUTING.md.
Join the Mattermost Contributors server to join community discussions about contributions, development, and more.

==========

Repository: sentry
URL: https://github.com/kavspvt2803/sentry
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Users and logs provide clues. Sentry provides answers.
  

What's Sentry?
Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.






Official Sentry SDKs

JavaScript
Electron
React-Native
Python
Ruby
PHP
Laravel
Go
Rust
Java/Kotlin
Objective-C/Swift
C#/F#
C/C++
Dart
Perl
Clojure
Elixir
Unity
Unreal Engine
PowerShell

Resources

Documentation
Discussions (Bugs, feature requests,
general questions)
Discord
Contributing
Bug Tracker
Code
Transifex (Translate
Sentry!)

==========

Repository: operating-system
URL: https://github.com/kavspvt2803/operating-system
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Home Assistant Operating System
Home Assistant Operating System (formerly HassOS) is a Linux based operating system optimized to host Home Assistant and its Add-ons.
Home Assistant Operating System uses Docker as its container engine. By default it deploys the Home Assistant Supervisor as a container. Home Assistant Supervisor in turn uses the Docker container engine to control Home Assistant Core and Add-Ons in separate containers. Home Assistant Operating System is not based on a regular Linux distribution like Ubuntu. It is built using Buildroot and it is optimized to run Home Assistant. It targets single board compute (SBC) devices like the Raspberry Pi or ODROID but also supports x86-64 systems with UEFI.

Features

Lightweight and memory-efficient
Minimized I/O
Over The Air (OTA) updates
Offline updates
Modular using Docker container engine

Supported hardware

Nabu Casa
Raspberry Pi
Hardkernel ODROID
Asus Tinker Board
Generic x86-64 (e.g. Intel NUC)
Virtual appliances

See the full list and specific models here
Getting Started
If you just want to use Home Assistant the official getting started guide and installation instructions take you through how to download Home Assistant Operating System and get it running on your machine.
If you're interested in finding out more about Home Assistant Operating System and how it works read on...
Development
If you don't have experience with embedded systems, Buildroot or the build process for Linux distributions it is recommended to read up on these topics first (e.g. Bootlin has excellent resources).
The Home Assistant Operating System documentation can be found on the Home Assistant Developer Docs website.
Components

Bootloader:

GRUB for devices that support UEFI
U-Boot for devices that don't support UEFI


Operating System:

Buildroot LTS Linux


File Systems:

SquashFS for read-only file systems (using LZ4 compression)
ZRAM for /tmp, /var and swap (using LZ4 compression)


Container Platform:

Docker Engine for running Home Assistant components in containers


Updates:

RAUC for Over The Air (OTA) and USB updates


Security:

AppArmor Linux kernel security module



Development builds
The Development build GitHub Action Workflow is a manually triggered workflow
which creates Home Assistant OS development builds. The development builds are
available at https://os-artifacts.home-assistant.io/index.html.

==========

Repository: ubicloud
URL: https://github.com/kavspvt2803/ubicloud
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Ubicloud   
Ubicloud is an open source cloud that can run anywhere. Think of it as an open alternative
to cloud providers, like what Linux is to proprietary operating systems.
Ubicloud provides IaaS cloud features on bare metal providers, such as Hetzner, Leaseweb,
and AWS Bare Metal. You can set it up yourself on these providers or you can use our
managed service.
Quick start
Managed platform
You can use Ubicloud without installing anything. When you do this, we pass along the
underlying provider's benefits to you, such as price or location.
https://console.ubicloud.com
Build your own cloud
You can also build your own cloud. To do this, start up Ubicloud's control plane and
connect to its cloud console.
git clone git@github.com:ubicloud/ubicloud.git

# Generate secrets for demo
./demo/generate_env

# Run containers: db-migrator, app (web & respirate), postgresql
docker-compose -f demo/docker-compose.yml up

# Visit localhost:3000

The control plane is responsible for cloudifying bare metal Linux machines.
The easiest way to build your own cloud is to lease instances from one of those
providers. For example: https://www.hetzner.com/sb
Once you lease instance(s), run the following script for each instance to cloudify
the instance. By default, the script cloudifies bare metal instances leased from
Hetzner. After you cloudify your instances, you can provision and manage cloud
resources on these machines.
# Enter hostname/IP and provider, and install SSH key as instructed by script
docker exec -it ubicloud-app ./demo/cloudify_server

Later when you create VMs, Ubicloud will assign them IPv6 addresses. If your ISP
doesn't support IPv6, please use a VPN or tunnel broker such as Mullvad or Hurricane
Electric's https://tunnelbroker.net/ to connect. Alternatively, you could lease
IPv4 addresses from your provider and add them to your control plane.
Why use it
Public cloud providers like AWS, Azure, and Google Cloud have made life easier for
start-ups and enterprises. But they are closed source, have you rent computers
at a huge premium, and lock you in. Ubicloud offers an open source alternative,
reduces your costs, and returns control of your infrastructure back to you. All
without sacrificing the cloud's convenience.
Today, AWS offers about two hundred cloud services. Ultimately, we will implement
10% of the cloud services that make up 80% of that consumption.
Example workloads and reasons to use Ubicloud today include:


You have an ephemeral workload like a CI/CD pipeline (we're integrating with
GitHub Actions), or you'd like to run compute/memory heavy tests. Our managed
cloud is ~3x cheaper than AWS, so you save on costs.


You want a portable and simple app deployment service like
Kamal. We're moving Ubicloud's control plane
from Heroku to Kamal; and we want to provide open and portable services for
Kamal's dependencies in the process.


You have bare metal machines sitting somewhere. You'd like to build your own
cloud for portability, security, or compliance reasons.


Status
You can provide us your feedback, get help, or ask us questions regarding your
Ubicloud installations in the Community Forum.
We follow an established architectural pattern in building public cloud services.
A control plane manages a data plane, where the data plane leverages open source
software.  You can find our current cloud components / services below.


Elastic Compute: Our control plane communicates with Linux bare metal servers
using SSH. We use Cloud
Hypervisor as our virtual
machine monitor (VMM); and each instance of the VMM is contained within Linux
namespaces for further isolation / security.


Networking: We use IPsec tunneling to
establish an encrypted and private network environment. We support IPv4 and IPv6 in
a dual-stack setup and provide both public and private networking. For security,
each customer’s VMs operate in their own networking namespace. For
firewalls
and load balancers,
we use Linux nftables.


Block Storage, non replicated: We use Storage Performance Development Toolkit
(SPDK) to provide virtualized block storage to VMs. SPDK enables
us to add enterprise features such as snapshot and replication in the future. We
follow security best practices and encrypt the data encryption key itself.


Attribute-Based Access Control (ABAC): With ABAC, you can define attributes,
roles, and permissions for users and give them fine-grained access to resources. You
can read more about our ABAC design here.


What's Next?: We're planning to work on a managed K8s or metrics/monitoring
service next. If you have a workload that would benefit from a specific cloud
service, please get in touch with us through our Community
Forum.


Control plane: Manages data plane services and resources. This is a Ruby program
that stores its data in Postgres. We use the Roda
framework to serve HTTP requests and Sequel to
access the database. We manage web authentication with
Rodauth. We communicate with data plane servers
using SSH, via the library net-ssh. For our
tests, we use RSpec.


Cloud console: Server-side web app served by the Roda framework. For the visual
design, we use Tailwind CSS with components from
Tailwind UI. We also use jQuery for interactivity.


If you’d like to start hacking with Ubicloud, any method of obtaining Ruby and Postgres
versions is acceptable. If you have no opinion on this, our development team uses asdf-vm
as documented here in detail.
Greptile provides an AI/LLM that indexes
Ubicloud's source code can answer questions about
it.
FAQ
Do you have any experience with building this sort of thing?
Our founding team comes from Azure; and worked at Amazon and Heroku before that.
We also have start-up experience. We were co-founders and founding team members
at Citus Data, which got acquired by
Microsoft.
How is this different than OpenStack?
We see three differences. First, Ubicloud is available as a managed service (vs boxed
software). This way, you can get started in minutes rather than weeks. Since Ubicloud
is designed for multi-tenancy, it comes with built-in features such as encryption
at rest and in transit, virtual networking, secrets rotation, etc.
Second, we're initially targeting developers. This -we hope- will give us fast feedback
cycles and enable us to have 6 key services in GA form in the next two years. OpenStack
is still primarily used for 3 cloud services.
Last, we're designing for simplicity. With OpenStack, you pick between 10 hypervisors,
10 S3 implementations, and 5 block storage implementations. The software needs to work
in a way where all of these implementations are compatible with each other. That leads
to consultant-ware. We'll take a more opinionated approach with Ubicloud.

==========

Repository: kanister
URL: https://github.com/kavspvt2803/kanister
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Kanister




Kanister is a data protection workflow management tool. It provides a set of
cohesive APIs for defining and curating data operations by abstracting away
tedious details around executing data operations on Kubernetes. It's extensible
and easy to install, operate and scale.
Highlights
✅ Kubernetes centric - Kanister's APIs are implemented as Custom Resource
Definitions
that conforms to Kubernetes' declarative management, security and distribution
models.
✅ Storage agnostic - Kanister allows you to efficiently and securely transfer
backup data between your services and the object storage service of your choice.
Use Kanister to backup, restore, and copy your data using your storage's APIs,
and Kanister won't get in the way.
✅ Asynchronous or synchronous task execution - Kanister can schedule your data
operation to run asynchronously in dedicated job pods, or synchronously via
Kubernetes apimachinery ExecStream framework.
✅ Re-usable workflow artifacts -  A Kanister blueprint can be re-used across
multiple workflows to protect different environment deployments.
✅ Extensible, atomic data operation functions - Kanister provides a collection
of easy-to-use
data operation functions that you can
add to your blueprint to express detailed backup and restore operation steps,
including pre-backup scaling down of replicas, working with all mounted volumes
in a pod etc.
✅ Secured via RBAC - Prevent unauthorized access to your workflows via Kubernetes
role-based access control
model.
✅ Observability - Kanister exposes logs, events and metrics to popular
observability tools like Prometheus, Grafana and Loki to provide you with
operational insights into your data protection workflows.
Quickstart
Follow the instructions in
the installation documentation, to
install Kanister on your Kubernetes cluster.
Walk through the tutorial to define,
curate and run your first data protection workflow using Kanister blueprints,
actionsets and profiles.
The examples directory contains many sample blueprints that you
can use to define data operations for:

AWS RDS
Cassandra
Couchbase
Elasticsearch
etcd
FoundationDB
K8ssandra
MongoDB
MongoDB on OpenShift using DeploymentConfig
MySQL
MySQL on OpenShift using DeploymentConfig
PostgreSQL
PostgreSQL on OpenShift using DeploymentConfig
Redis

The Kanister architecture is documented
here.
Getting Help
If you have any questions or run into issues, feel free to reach out to us on
Slack.
GitHub issues or pull requests that have been inactive for more than 60 days
will be labeled as stale. If they remained inactive for another 30 days, they
will be automatically closed. To be exempted from the issue lifecycle, discuss
with a maintainer the reasons behind the exemption, and add the frozen label
to the issue or pull request.
If you discovered any security issues, refer to our SECURITY.md
documentation for our security policy, including steps on how to report
vulnerabilities.
Community
The Kanister community meetings happen once every two weeks on Thursday, 16:00
UTC, where we discuss ongoing interesting features, issues, and pull requests.
Come join us! Everyone is welcome! 🙌 (Zoom link is bookmarked on Slack.)
If you are currently using Kanister, we would love to hear about it! Feel free
to add your organization to the ADOPTERS.md by submitting a
pull request.
Code of Conduct
Kanister is for everyone. We ask that our users and contributors take a few
minutes to review our Code of Conduct.
Contributing to Kanister
We welcome contributions to Kanister! If you're interested in getting involved, please take a look at our guidelines:


BUILD.md: Contains detailed instructions on how to build and test Kanister locally or within a CI/CD pipeline.  Please refer to this guide if you want to make changes to Kanister's codebase.
Build and Test Instructions


CONTRIBUTING.md:  Provides essential information on how to contribute code, documentation, or bug reports, as well as our coding style and commit message conventions.
Contribution Guidelines


Resources

CNCF - Enhancing data protection workflows with Kanister and Argo workflows
CNCF - Kanister: Application-Level Data Protection on Kubernetes
CNCF - Integrating Backup Into Your GitOps CI/CD Pipeline
DoK - Kanister & Kopia: An Open-Source Data Protection Match Made in Heaven
DoK - Kanister: Application-Level Data Operations on Kubernetes
Kanister Overview 2021 
SIG Apps Demo
Percona Live 2018

License
Apache License 2.0, see LICENSE.

==========

Repository: netdata
URL: https://github.com/kavspvt2803/netdata
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
X-Ray Vision for your infrastructure!
Every Metric, Every Second. No BS.














Visit the Project's Home Page

MENU: GETTING STARTED | HOW IT WORKS | FAQ | DOCS | COMMUNITY | CONTRIBUTE | LICENSE

Important 💡
People get addicted to Netdata. Once you use it on your systems, there's no going back!


TL;DR
Netdata is an open-source, real-time infrastructure monitoring platform designed for instant visibility and proactive troubleshooting across your entire IT environment. It captures every metric, every second, providing detailed insights into systems, containers, applications, and logs without compromising performance or requiring complex setup.
Key Advantages:

Instant InsightsReal-time, per-second metrics and visualizations for rapid problem detection.
Automated and Zero-ConfigurationEasy deployment with immediate monitoring—no complex setup needed.
ML-Driven IntelligenceBuilt-in machine learning automatically detects anomalies, predicts issues, and assists in root-cause analysis.
Highly EfficientProven minimal resource usage, exceptional scalability, and best-in-class energy efficiency validated by independent research.
Distributed & SecureData stays securely within your infrastructure; no centralization required.

Netdata complements or replaces traditional monitoring tools, offering significant performance and usability advantages over Prometheus, Datadog, Dynatrace, and similar products, while remaining fully compatible and integration-friendly.
Designed for organizations seeking simplified operations, reduced overhead, and cost-effective monitoring solutions, Netdata provides a comprehensive, scalable, and user-friendly approach to observability.
✨ Key Features:


Real-Time
Per-second data collection and real-time processing provides immediate visibility into your infrastructure's behavior.
Unique: Netdata works in a beat, and everything happens at this pace. You hit Enter in the terminal, and just a second later, the result appears on the dashboard.


Zero-Configuration
Start monitoring in minutes with automatic detection and discovery, fully automated dashboards, and hundreds of pre-configured alerts.
Unique: Netdata auto-discovers everything on the nodes it runs. All kernel technologies, all processes, all applications, all containers, all hardware components. And with its dynamic configuration, any changes can be done via the dashboard.


ML-Powered
Unsupervised anomaly detection and pattern recognition for all metrics, providing advanced correlations and instant root cause analysis.
Unique: Netdata trains multiple true ML models per metric, at the edge, for all metrics! 
Unique: A scoring engine identifies correlations across metrics, applications, nodes, services, even cloud providers and data centers!


Long-Term Retention
High-performance and efficient tiered storage for years of retention and fast query responses.
Unique: Netdata needs ~0.5 per sample on disk, offering superb compression for high-resolution data! 
Unique: A tiered storage engine automatically downsamples old data for archiving, long term retention and capacity planning.


Advanced Visualization
Rich, interactive low-latency dashboards for deep system and applications insights and rapid troubleshooting.
Unique: Netdata dashboards allow you to slice and dice any dataset, without learning a query language. 
Unique: A multi-faceted query engine, analyzes all aspects of your data in one go, and the dashboard provides interactive analysis for all them (NIDL framework).


Extreme Scalability
Native horizontal scalability, while maintaining performance and ease of use.
Unique: Simple Parent-Child centralization and native horizontal scalability. 
Unique: Each Netdata can scale to multi-million samples/s with best in-class resources utilization.


Complete End-to-End Visibility
From infrastructure to applications, logs to metrics, hardware to databases, all in one solution.
Unique: Netdata is built to simplify your operations, empower your team, provide clarity and eliminate silos.


Edge-Based
All processing and storage of your data, at your premises, as close to the edge as possible.
Unique: Instead of centralizing observability data, Netdata distributes the code. This provides higher processing capacity by utilizing resources that are usually available and spare, while eliminating most of the cost involved for metrics and logs management.



The Netdata Ecosystem

Note: This repository contains the Netdata Agent, the open-source core of the Netdata ecosystem. For information about other components, see below.

This three-part architecture enables Netdata to scale seamlessly from single-node deployments to complex multi-cloud environments with thousands of nodes, supporting long-term data retention without compromising performance.



Component
Description
License




Netdata Agent
• The heart of Netdata's monitoring capabilities• Handles data collection, storage, querying, ML analysis, exports, and alerts• Runs on physical/virtual servers, cloud, Kubernetes, and IoT devices• Optimized for zero production impact• Core of all observability features
GPL v3+


Netdata Cloud
• Adds enterprise-grade features (user management and RBAC, horizontal scalability,  centralized alert management, access from anywhere)• Available as SaaS or on-premises• Includes free community tier• Does not centralize metric storage



Netdata UI
• Powers all dashboards and visualizations• Free to use with both Agent and Cloud• Included in standard Netdata packages• Latest version available via CDN
NCUL1



Key capabilities of the Netdata Agent
With these capabilities, Netdata Agent provides a powerful, automated monitoring solution that works right out-of-the-box while remaining highly customizable for specific needs.



Capability
Description




Comprehensive Data Collection
• 800+ integrations out of the box• Collects metrics from systems, containers, VMs, hardware sensors• Supports OpenMetrics exporters, StatsD, and logs• OpenTelemetry support coming soon


Performance & Precision
• Per-second data collection• Real-time visualization with 1-second latency• High-resolution metrics for precise monitoring


Edge-Based ML
• Trains ML models directly at the edge• Automatic anomaly detection per metric• Pattern recognition based on historical behavior


Advanced Log Management
• Direct systemd-journald and Windows Event Log integrations• Tools for log conversion (log2journal, systemd-cat-native)• Process logs at the edge - no centralization needed• Rich log visualization dashboards


Observability Pipeline
• Build Parent-Child relationships between Agents• Create flexible centralization points• Control data replication and retention at multiple levels


Automated Visualization
• NIDL (Nodes, Instances, Dimensions & Labels) data model• Auto-generated, correlated dashboards• Filter and analyze data without query language• Free to use, powered by Netdata UI


Smart Alerting
• Hundreds of pre-configured alerts• Detect common issues automatically• Multiple notification methods• Proactive problem detection


Low Maintenance
• Auto-detection of metrics• Zero-touch machine learning• Easy scalability• CI/CD friendly deployment


Open & Extensible
• Modular architecture• Easy to extend and customize• Integrates with existing monitoring tools• Active community ecosystem



What can be monitored with the Netdata Agent
Netdata monitors all the following:



Component
Linux
FreeBSD
macOS
Windows




System ResourcesCPU, Memory and system shared resources
Full
Yes
Yes
Yes


StorageDisks, Mount points, Filesystems, RAID arrays
Full
Yes
Yes
Yes


NetworkNetwork Interfaces, Protocols, Firewall, etc
Full
Yes
Yes
Yes


Hardware & SensorsFans, Temperatures, Controllers, GPUs, etc
Full
Some
Some
Some


O/S ServicesResources, Performance and Status
Yessystemd
-
-
-


ProcessesResources, Performance, OOM, and more
Yes
Yes
Yes
Yes


System and Application Logs
Yessystemd-journal
-
-
YesWindows Event Log, andEvent Tracing for Windows


Network ConnectionsLive TCP and UDP sockets per PID
Yes
-
-
-


ContainersDocker/containerd, LXC/LXD, Kubernetes, etc
Yes
-
-
-


VMs (from the host)KVM, qemu, libvirt, Proxmox, etc
Yescgroups
-
-
YesHyper-V


Synthetic ChecksTest APIs, TCP ports, Ping, Certificates, etc
Yes
Yes
Yes
Yes


Packaged Applicationsnginx, apache, postgres, redis, mongodb,and hundreds more
Yes
Yes
Yes
Yes


Cloud Provider InfrastructureAWS, GCP, Azure, and more
Yes
Yes
Yes
Yes


Custom ApplicationsOpenMetrics, StatsD and soon OpenTelemetry
Yes
Yes
Yes
Yes



When operating on Linux, the Netdata Agent continuously monitors every available kernel feature and all hardware sensors for errors. This covers Intel, AMD, and Nvidia GPUs, PCI Advanced Error Reporting (PCI AER), RAM Error Detection and Correction (RAM EDAC), Intelligent Platform Management Interface (IPMI), S.M.A.R.T. for disks, Intel Running Average Power Limit (Intel RAPL), NVMe disks, as well as fans, power supplies, voltage readings, and more.

⭐ Netdata is the most energy-efficient monitoring tool ⭐








Dec 11, 2023: University of Amsterdam published a study related to the impact of monitoring tools for Docker based systems, aiming to answer 2 questions:

The impact of monitoring on the energy efficiency of Docker-based systems
The impact of monitoring on Docker-based systems?


🚀 Netdata excels in energy efficiency: "... Netdata is the most energy-efficient tool ...", as the study says.
🚀 Netdata excels in CPU Usage, RAM Usage and Execution Time, and has a similar impact on Network Traffic as Prometheus.

The study didn’t normalize the results based on the number of metrics collected. Given that Netdata usually collects significantly more metrics than the other tools, Netdata managed to outperform the other tools, while ingesting a much higher number of metrics. Read the full study here.

Netdata vs Prometheus 2025 Review








NEW! On the same workload, Netdata uses 1/3rd less CPU, consumes 1/8th of the RAM, performes 31 times less disk I/O and stores 40 times more data while being up to 22 times faster in query responses! Read the full 2025 review in our blog.

 




  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF)
   
  ...and due to your love ❤️, it is one of the most ⭐'d projects in the CNCF landscape!

 


You can see Netdata live!
FRANKFURT |
	NEWYORK |
	ATLANTA |
	SANFRANCISCO |
	TORONTO |
	SINGAPORE |
	BANGALORE

We've set up multiple demo clusters around the world, each running with the default configuration and showing real monitoring data.

Choose the instance closest to you for the best experience.


Getting Started











1. Install Netdata everywhere ✌️
Netdata can be installed on all Linux, macOS, FreeBSD and Windows systems. We provide binary packages for the most popular operating systems and package managers.

Install on Ubuntu, Debian CentOS, Fedora, Suse, Red Hat, Arch, Alpine, Gentoo, even BusyBox.
Install with Docker.
Netdata is a Verified Publisher on DockerHub and our users enjoy free unlimited DockerHub pulls 😍.
Install on macOS 🤘.
Install on FreeBSD and pfSense.
Install on Windows.
Install from source 
For Kubernetes deployments check here.

Check also the Netdata Deployment Guides to decide how to deploy it in your infrastructure.
By default, you will have immediately available a local dashboard. Netdata starts a web server for its dashboard at port 19999. Open up your web browser of choice and
navigate to http://NODE:19999, replacing NODE with the IP address or hostname of your Agent. If installed on localhost, you can access it through http://localhost:19999.
Note: the binary packages we provide, install Netdata UI automatically. Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud.
2. Configure Collectors 💥
Netdata auto-detects and auto-discovers most operating system data sources and applications. However, many data sources require some manual configuration, usually to allow Netdata to get access to the metrics.

For a detailed list of the 800+ collectors available, check this guide.
To monitor SNMP devices, check this guide.

3. Configure Alert Notifications 🔔
Netdata comes with hundreds of pre-configured alerts that automatically check your metrics immediately after they start getting collected.
Netdata can dispatch alert notifications to multiple third party systems, including: email, Alerta, AWS SNS, Discord, Dynatrace, flock, gotify, IRC, Matrix, MessageBird, Microsoft Teams, ntfy, OPSgenie, PagerDuty, Prowl, PushBullet, PushOver, RocketChat, Slack, SMS tools, Syslog, Telegram, Twilio.
By default, Netdata will send e-mail notifications if there is a configured MTA on the system.
4. Configure Netdata Parents 👪
Optionally, configure one or more Netdata Parents. A Netdata Parent is a Netdata Agent that has been configured to accept streaming connections from other Netdata Agents.
Netdata Parents provide:


Infrastructure level dashboards, at http://parent.server.ip:19999/.
Each Netdata Agent has an API listening at the TCP port 19999 of each server.
When you hit that port with a web browser (e.g. http://server.ip:19999/), the Netdata Agent UI is presented.
When the Netdata Agent is also a Parent, the UI of the Parent includes data for all nodes that stream metrics to that Parent.


Increased retention for all metrics of all your nodes.
Each Netdata Agent maintains each own database of metrics. But Parents can be given additional resources to maintain a much longer database than
individual Netdata Agents.


Central configuration of alerts and dispatch of notifications.
Using Netdata Parents, all the alert notifications integrations can be configured only once at the Parent and they can be disabled at the Netdata Agents.


You can also use Netdata Parents to:

Offload your production systems (the parents run ML, alerts, queries, etc. for all their children)
Secure your production systems (the parents accept user connections for all their children)

5. Connect to Netdata Cloud ☁️
Sign-in to Netdata Cloud and connect your Netdata Agents and Parents.
If you connect your Netdata Parents, there is no need to connect your Netdata Agents. They will be connected via the Parents.
When your Netdata nodes are connected to Netdata Cloud, you can (on top of the above):

Access your Netdata Agents from anywhere
Access sensitive Netdata Agent features (like "Netdata Functions": processes, systemd-journal)
Organize your infra in spaces and Rooms
Create, manage, and share custom dashboards
Invite your team and assign roles to them (Role-Based Access Control)
Get infinite horizontal scalability (multiple independent Netdata Agents are viewed as one infra)
Configure alerts from the UI
Configure data collection from the UI
Netdata Mobile App notifications

🤟 Netdata Cloud doesn’t prevent you from using your Netdata Agents and Parents directly, and vice versa.
👌 Your metrics are still stored in your network when you connect your Netdata Agents and Parents to Netdata Cloud.

How it works
Netdata is built around a modular metrics processing pipeline.
Click to see more details about this pipeline...
 
Each Netdata Agent can perform the following functions:


COLLECT metrics from their sources
Uses internal and external plugins to collect data from their sources.
Netdata auto-detects and collects almost everything from the operating system: including CPU, Interrupts, Memory, Disks, Mount Points, Filesystems, Network Stack, Network Interfaces, Containers, VMs, Processes, systemd units, Linux Performance Metrics, Linux eBPF, Hardware Sensors, IPMI, and more.
It collects application metrics from applications: PostgreSQL, MySQL/MariaDB, Redis, MongoDB, Nginx, Apache, and hundreds more.
Netdata also collects your custom application metrics by scraping OpenMetrics exporters, or via StatsD.
It can convert web server log files to metrics and apply ML and alerts to them in real-time.
And it also supports synthetic tests / white box tests, so you can ping servers, check API responses, or even check filesystem files and directories to generate metrics, train ML and run alerts and notifications on their status.


STORE metrics to a database
Uses database engine plugins to store the collected data, either in memory and/or on disk. We have developed our own dbengine for storing the data in a very efficient manner, allowing Netdata to have less than one byte per sample on disk and amazingly fast queries.


LEARN the behavior of metrics (ML)
Trains multiple Machine-Learning (ML) models per metric to learn the behavior of each metric individually. Netdata uses the kmeans algorithm and creates by default a model per metric per hour, based on the values collected for that metric over the last 6 hours. The trained models are persisted to disk.


DETECT anomalies in metrics (ML)
Uses the trained machine learning (ML) models to detect outliers and mark collected samples as anomalies. Netdata stores anomaly information together with each sample and also streams it to Netdata Parents so that the anomaly is also available at query time for the whole retention of each metric.


CHECK metrics and trigger alert notifications
Uses its configured alerts (you can configure your own) to check the metrics for common issues and uses notification plugins to send alert notifications.


STREAM metrics to other Netdata Agents
Push metrics in real-time to Netdata Parents.


ARCHIVE metrics to third party databases
Export metrics to industry standard time-series databases, like Prometheus, InfluxDB, OpenTSDB, Graphite, etc.


QUERY metrics and present dashboards
Provide an API to query the data and present interactive dashboards to users.


SCORE metrics to reveal similarities and patterns
Score the metrics according to the given criteria, to find the needle in the haystack.


When using Netdata Parents, all the functions of a Netdata Agent (except data collection) can be delegated to Parents to offload production systems.
The core of Netdata is developed in C. We have our own libnetdata, that provides:


DICTIONARY
A high-performance algorithm to maintain both indexed and ordered pools of structures Netdata needs. It uses JudyHS arrays for indexing, although it is modular: any hashtable or tree can be integrated into it. Despite being in C, dictionaries follow object-oriented programming principles, so there are constructors, destructors, automatic memory management, garbage collection, and more. For more, see here.


ARAL
ARray ALlocator (ARAL) is used to minimize the system allocations made by Netdata. ARAL is optimized for maximum multithreaded performance. It also allows all structures that use it to be allocated in memory-mapped files (shared memory) instead of RAM. For more, see here.


PROCFILE
A high-performance /proc (but also any) file parser and text tokenizer. It achieves its performance by keeping files open and adjusting its buffers to read the entire file in one call (which is also required by the Linux kernel). For more, see here.


STRING
A string internet mechanism, for string deduplication and indexing (using JudyHS arrays), optimized for multithreaded usage. For more, see here.


ARL
Adaptive Resortable List (ARL) is a very fast list iterator, that keeps the expected items on the list in the same order they are found in an input list. So, the first iteration is somewhat slower, but all the following iterations are perfectly aligned for the best performance. For more, see here.


BUFFER
A flexible text buffer management system that allows Netdata to automatically handle dynamically sized text buffer allocations. The same mechanism is used for generating consistent JSON output by the Netdata APIs. For more, see here.


SPINLOCK
Like POSIX MUTEX and RWLOCK but a lot faster, based on atomic operations, with significantly smaller memory impact, while being portable.


PGC
A caching layer that can be used to cache any kind of time-related data, with automatic indexing (based on a tree of JudyL arrays), memory management, evictions, flushing, pressure management. This is extensively used in dbengine. For more, see here.


The above, and many more, allow Netdata developers to work on the application fast and with confidence. Most of the business logic in Netdata is a work of mixing the above.
Netdata data collection plugins can be developed in any language. Most of our application collectors though are developed in Go.

FAQ
🛡️ Is Netdata secure?
Of course, it is! We do our best to ensure it is!
Click to see detailed answer ...
  
We understand that Netdata is a software piece installed on millions of production systems across the world. So, it is important for us, Netdata to be as secure as possible:

We follow the Open Source Security Foundation best practices.
We have given great attention to detail when it comes to security design. Check out our security design.
Netdata is a popular open-source project and is frequently tested by many security analysts.
Check also our security policies and advisories published so far.

  

🌀 Will Netdata consume significant resources on my servers?
No, it will not! We promise this will be fast!
Click to see detailed answer ...
  
Although each Netdata Agent is a complete monitoring solution packed into a single application, and despite the fact that Netdata collects every metric every single second and trains multiple ML models per metric, you will find that Netdata has amazing performance! In many cases, it outperforms other monitoring solutions that have significantly fewer features or far smaller data collection rates.
This is what you should expect:


For production systems, each Netdata Agent with default settings (everything enabled, ML, Health, DB) should consume about 5% CPU utilization of one core and about 150 MiB or RAM.
By using a Netdata parent and streaming all metrics to that parent, you can disable ML & health and use an ephemeral DB (like alloc) on the children, leading to utilization of about 1% CPU of a single core and 100 MiB of RAM. Of course, these depend on how many metrics are collected.


For Netdata Parents, for about 1 to 2 million metrics, all collected every second, we suggest a server with 16 cores and 32GB RAM. Less than half of it will be used for data collection and ML. The rest will be available for queries.


Netdata has extensive internal instrumentation to help us reveal how the resources consumed are used. All these are available in the "Netdata Monitoring" section of the dashboard. Depending on your use case, there are many options to optimize resource consumption.
Even if you need to run Netdata on extremely weak embedded or IoT systems, you will find that Netdata can be tuned to be very performant.
  

📜 How much retention can I have?
As much as you need!
Click to see detailed answer ...
  
Netdata supports tiering, to downsample past data and save disk space. With default settings, it has three tiers:

tier 0, with high resolution, per-second, data.
tier 1, mid-resolution, per minute, data.
tier 2, low-resolution, per hour, data.

All tiers are updated in parallel during data collection. Increase the disk space you give to Netdata to get a longer history for your metrics. Tiers are automatically chosen at query time depending on the time frame and the resolution requested.
  

🚀 Does it scale? I really have a lot of servers!
Netdata is designed to scale and can handle large volumes of data.
Click to see detailed answer ...
  
Netdata is a distributed monitoring solution. You can scale it to infinity by spreading Netdata Agents across your infrastructure.
With the streaming feature of the Agent, we can support monitoring ephemeral servers but also allow the creation of "monitoring islands" where metrics are aggregated to a few servers (Netdata Parents) for increased retention, or for offloading production systems.


✈️ Netdata Parents provide great vertical scalability, so you can have as big parents as the CPU, RAM and Disk resources you can dedicate to them. In our lab, we constantly stress test Netdata Parents with several million metrics collected per second, to ensure it is reliable, stable, and robust at scale.


🚀 In addition, Netdata Cloud provides virtually unlimited horizontal scalability. It "merges" all the Netdata parents you have into one unified infrastructure at query time. Netdata Cloud itself is probably the biggest single installation monitoring platform ever created, currently monitoring about 100k online servers with about 10k servers changing state (added/removed) per day!


Example: the following chart comes from a single Netdata Parent. As you can see on it, 244 nodes stream to it metrics of about 20k running containers. On this specific chart, there are three dimensions per container, so a total of about 60k time-series queries are executed to present it.

  

💾 My production servers are very sensitive in disk I/O. Can I use Netdata?
Yes, you can!
Click to see detailed answer ...
  
The Netdata Agent has been designed to spread disk writes across time. Each metric is flushed to disk every 17 minutes (1000 seconds), but metrics are flushed evenly across time, at an almost constant rate. Also, metrics are packed into bigger blocks we call extents and are compressed with ZSTD before saving them, to minimize the number of I/O operations made.
The Netdata Agent also employs direct I/O for all its database operations. By managing its own caches, Netdata avoids overburdening system caches, facilitating a harmonious coexistence with other applications.
Single node Agents (not Parents), should have a constant write rate of about 50 KiB/s or less, with some spikes above that every minute (flushing of tier 1) and higher spikes every hour (flushing of tier 2).
Health Alerts and Machine-Learning run queries to evaluate their expressions and learn from the metrics' patterns. These are also spread over time, so there should be an almost constant read rate too.
To make Netdata not use the disks at all, we suggest the following:

Use database mode alloc or ram to disable writing metric data to disk.
Configure streaming to push in real-time all metrics to a Netdata Parent. The Netdata Parent will maintain metrics on disk for this node.
Disable ML and health on this node. The Netdata Parent will do them for this node.
Use the Netdata Parent to access the dashboard.

Using the above, the Netdata Agent on your production system will not use a disk.
  

🤨 How is Netdata different from a Prometheus and Grafana setup?
Netdata is a "ready to use" monitoring solution. Prometheus and Grafana are tools to build your own monitoring solution.
Netdata is also a lot faster, requires significantly fewer resources and puts almost no stress on the server it runs. For a performance comparison check this blog.
Click to see detailed answer ...
  
First, we have to say that Prometheus as a time-series database and Grafana as a visualizer are excellent tools for what they do.
However, we believe that such a setup is missing a key element: A Prometheus and Grafana setup assumes that you know everything about the metrics you collect, and you understand deeply how they’re structured, they should be queried and visualized.
In reality, this setup has a lot of problems. The vast number of technologies, operating systems, and applications we use in our modern stacks makes it impossible for any single person to know and understand everything about anything. We get testimonials regularly from Netdata users across the biggest enterprises, that Netdata manages to reveal issues, anomalies and problems they weren’t aware of, and they didn't even have the means to find or troubleshoot.
So, the biggest difference of Netdata to Prometheus, and Grafana, is that we decided that the tool needs to have a much better understanding of the components, the applications, and the metrics it monitors.


When compared to Prometheus, Netdata needs for each metric much more than just a name, some labels, and a value over time. A metric in Netdata is a structured entity that correlates with other metrics in a certain way and has specific attributes that depict how it should be organized, treated, queried, and visualized. We call this the NIDL (Nodes, Instances, Dimensions, Labels) framework.
Maintaining such an index is a challenge: first, because the raw metrics collected do not provide this information, so we have to add it, and second because we need to maintain this index for the lifetime of each metric, which with our current database retention, it is usually more than a year.
At the same time, Netdata provides better retention than Prometheus due to database tiering, scales easier than Prometheus due to streaming, supports anomaly detection, and it has a metrics scoring engine to find the needle in the haystack when needed.


When compared to Grafana, Netdata is fully automated. Grafana has more customization capabilities than Netdata, but Netdata presents fully functional dashboards by itself, and most importantly, it gives you the means to understand, analyze, filter, slice and dice the data without the need for you to edit queries or be aware of any peculiarities the underlying metrics may have.
Furthermore, to help you when you need to find the needle in the haystack, Netdata has advanced troubleshooting tools provided by the Netdata metrics scoring engine, that allows it to score metrics based on their anomaly rate, their differences or similarities for any given time frame.


Still, if you’re already familiar with Prometheus and Grafana, Netdata integrates nicely with them, and we have reports from users who use Netdata with Prometheus and Grafana in production.
  

🤨 How is Netdata different from DataDog, New Relic, Dynatrace, X SaaS Provider?
With Netdata your data are always on-prem and your metrics are always high-resolution.
Click to see detailed answer ...
  
Most commercial monitoring providers face a significant challenge: they centralize all metrics to their infrastructure, and this is, inevitably, expensive. It leads them to one or more of the following:

be unrealistically expensive
limit the number of metrics they collect
limit the resolution of the metrics they collect

As a result, they try to find a balance: collect the least possible data, but collect enough to have something useful out of it.
We, at Netdata, see monitoring in a completely different way: monitoring systems should be built bottom-up and be rich in insights, so we focus on each component individually to collect, store, check and visualize everything related to each of them, and we make sure that all components are monitored. Each metric is important.
This is why Netdata trains multiple machine-learning models per metric, based exclusively on their own past (no sampling of data, no sharing of trained models) to detect anomalies based on the specific use case and workload each component is used.
This is also why Netdata alerts are attached to components (instances) and are configured with dynamic thresholds and rolling windows, instead of static values.
The distributed nature of Netdata helps scale this approach: your data is spread inside your infrastructure, as close to the edge as possible. Netdata is not one data lane. Each Netdata Agent is a data lane, and all of them together build a massive distributed metrics processing pipeline that ensures all your infrastructure components and applications are monitored and operating as they should.
  

🤨 How is Netdata different from Nagios, Icinga, Zabbix, etc.?
Netdata offers real-time, comprehensive monitoring and the ability to monitor everything without any custom configuration required.
Click to see detailed answer ...
  
While Nagios, Icinga, Zabbix, and other similar tools are powerful and highly customizable, they can be complex to set up and manage. Their flexibility often comes at the cost of ease-of-use, especially for users who aren’t systems administrators or don’t have extensive experience with these tools. Additionally, these tools generally require you to know what you want to monitor in advance and configure it explicitly.
Netdata, on the other hand, takes a different approach. It provides a "ready to use" monitoring solution with a focus on simplicity and comprehensiveness. It automatically detects and starts monitoring many different system metrics and applications out-of-the-box, without any need for custom configuration.
In comparison to these traditional monitoring tools, Netdata:


Provides real-time, high-resolution metrics, as opposed to the often minute-level granularity that tools like Nagios, Icinga, and Zabbix provide.


Automatically generates meaningful, organized, and interactive visualizations of the collected data. Unlike other tools, where you have to manually create and organize graphs and dashboards, Netdata takes care of this for you.


Applies machine learning to each individual metric to detect anomalies, providing more insightful and relevant alerts than static thresholds.


Designed to be distributed, so your data is spread inside your infrastructure, as close to the edge as possible. This approach is more scalable and avoids the potential bottleneck of a single centralized server.


Has a more modern and user-friendly interface, allowing anyone, not just experienced administrators, to easily assess the health and performance of their systems.


Even if you're already using Nagios, Icinga, Zabbix, or similar tools, you can use Netdata alongside them to augment your existing monitoring capabilities with real-time insights and user-friendly dashboards.
  

😳 I feel overwhelmed by the amount of information in Netdata. What should I do?
Netdata is designed to provide comprehensive insights, but we understand that the richness of information might sometimes feel overwhelming. Here are some tips on how to navigate and use Netdata effectively...
Click to see detailed answer ...
  
Netdata is indeed a very comprehensive monitoring tool. It's designed to provide you with as much information as possible about your system and applications, so that you can understand and address any issues that arise. However, we understand that the sheer amount of data can sometimes be overwhelming.
Here are some suggestions on how to manage and navigate this wealth of information:


Start with the Metrics Dashboard
Netdata's Metrics Dashboard provides a high-level summary of your system's status. We have added summary tiles on almost every section, you reveal the information that is more important. This is a great place to start, as it can help you identify any major issues or trends at a glance.


Use the Search Feature
If you're looking for specific information, you can use the search feature to find the relevant metrics or charts. This can help you avoid scrolling through all the data.


Customize your Dashboards
Netdata allows you to create custom dashboards, which can help you focus on the metrics that are most important to you. Sign-in to Netdata and there you can have your custom dashboards. (coming soon to the Agent dashboard too)


Leverage Netdata's Anomaly Detection
Netdata uses machine learning to detect anomalies in your metrics. This can help you identify potential issues before they become major problems. We have added an AR button above the dashboard table of contents to reveal the anomaly rate per section so that you can spot what could need your attention.


Take Advantage of Netdata's Documentation and Blogs
Netdata has extensive documentation that can help you understand the different metrics and how to interpret them. You can also find tutorials, guides, and best practices there.


Remember, it's not necessary to understand every single metric or chart right away. Netdata is a powerful tool, and it can take some time to fully explore and understand all of its features. Start with the basics and gradually delve into more complex metrics as you become more comfortable with the tool.
  

☁️ Do I have to subscribe to Netdata Cloud?
Netdata Cloud delivers the full suite of features and functionality that Netdata offers, including a free community tier.
While our default onboarding process encourages users to take advantage of Netdata Cloud, including a complimentary one-month trial of our full business product, it is not mandatory. Users can bypass this process entirely and still use the Netdata Agents along with the Netdata UI, without the need to sign up for Netdata Cloud.
Click to see detailed answer ...
  
The Netdata Agent dashboard and the Netdata Cloud dashboard are the same. Still, Netdata Cloud provides additional features that the Netdata Agent is not capable of. These include:

Access your infrastructure from anywhere.
Have SSO to protect sensitive features.
Customizable (custom dashboards and other settings are persisted when you’re signed in to Netdata Cloud)
Configuration of Alerts and Data Collection from the UI
Security (Role-Based Access Control).
Horizontal Scalability ("blend" multiple independent parents in one uniform infrastructure)
Central Dispatch of Alert Notifications (even when multiple independent parents are involved)
Mobile App for Alert Notifications

We encourage you to support Netdata by buying a Netdata Cloud subscription. A successful Netdata is a Netdata that evolves and gets improved to provide simpler, faster and easier monitoring for all of us.
For organizations that need a fully on-prem solution, we provide Netdata Cloud for on-prem installation. Contact us for more information.
  

🔎 What does the anonymous telemetry collected by Netdata entail?
Your privacy is our utmost priority. As part of our commitment to improving Netdata, we rely on anonymous telemetry data from our users who choose to leave it enabled. This data greatly informs our decision-making processes and contributes to the future evolution of Netdata.
Should you wish to disable telemetry, instructions for doing so are provided in our installation guides.
Click to see detailed answer ...
  
Netdata is in a constant state of growth and evolution. The decisions that guide this development are ideally rooted in data. By analyzing anonymous telemetry data, we can answer questions such as "What features are being used frequently?", "How do we prioritize between potential new features?" and "What elements of Netdata are most important to our users?"
By leaving anonymous telemetry enabled, users indirectly contribute to shaping Netdata's roadmap, providing invaluable information that helps us prioritize our efforts for the project and the community.
We are aware that for privacy or regulatory reasons, not all environments can allow telemetry. To cater to this, we’ve simplified the process of disabling telemetry:

During installation, you can append --disable-telemetry to our kickstart.sh script, or
Create the file /etc/netdata/.opt-out-from-anonymous-statistics and then restart Netdata.

These steps will disable the anonymous telemetry for your Netdata installation.
Please note, even with telemetry disabled, Netdata still requires a Netdata Registry for alert notifications' Call To Action (CTA) functionality. When you click an alert notification, it redirects you to the Netdata Registry, which then directs your web browser to the specific Netdata Agent that issued the alert for further troubleshooting. The Netdata Registry learns the URLs of your Agents when you visit their dashboards.
Any Netdata Agent can act as a Netdata Registry. Designate one Netdata Agent as your Registry, read more here.
  

😏 Who uses Netdata?
Netdata is a widely adopted project...
Click to see detailed answer ...
  
Browse the Netdata stargazers on GitHub to discover users from renowned companies and enterprises, such as ABN AMRO Bank, AMD, Amazon, Baidu, Booking.com, Cisco, Delta, Facebook, Google, IBM, Intel, Logitech, Netflix, Nokia, Qualcomm, Realtek Semiconductor Corp, Redhat, Riot Games, SAP, Samsung, Unity, Valve, and many others.
Netdata also enjoys significant usage in academia, with notable institutions including New York University, Columbia University, New Jersey University, Seoul National University, University College London, among several others.
And, Netdata is also used by many governmental organizations worldwide.
In a nutshell, Netdata proves invaluable for:


Infrastructure intensive organizations
Such as hosting/cloud providers and companies with hundreds or thousands of nodes, who require a high-resolution, real-time monitoring solution for a comprehensive view of all their components and applications.


Technology operators
Those in need of a standardized, comprehensive solution for round-the-clock operations. Netdata not only facilitates operational automation and provides controlled access for their operations engineers, but also enhances skill development over time.


Technology startups
Who seek a feature-rich monitoring solution from the get-go.


Freelancers
Who seek a simple, efficient and straightforward solution without sacrificing performance and outcomes.


Professional SysAdmins and DevOps
Who appreciate the fine details and understand the value of holistic monitoring from the ground up.


Everyone else
All of us, who are tired of the inefficiency in the monitoring industry and would love a refreshing change and a breath of fresh air. 🙂


  

🌐 Is Netdata open-source?
The Netdata Agent is open-source, but the overall Netdata ecosystem is a hybrid solution, combining open-source and closed-source components.
Click to see detailed answer ...
  
Open-source is about sharing intellectual property with the world, and at Netdata, we embrace this philosophy wholeheartedly.
The Netdata Agent, the core of our ecosystem and the engine behind all our observability features, is fully open-source. Licensed under GPLv3+, the Netdata Agent represents our commitment to open-sourcing innovation in a wide array of observability technologies, including data collection, database design, query engines, observability data modeling, machine learning and unsupervised anomaly detection, high-performance edge computing, real-time monitoring, and more.
The Netdata Agent is our gift to the world, ensuring that the cutting-edge advancements we've developed are freely accessible to everyone.
However, as a privately funded company, we also need to monetize our open-source software to demonstrate product-market fit and sustain our growth.
Traditionally, open-source projects have often used the open-core model, where a basic version of the software is open-source, and additional features are reserved for a commercial, closed-source version. This approach can limit access to advanced innovations, as most of these remain closed-source.
At Netdata, we take a slightly different path. We don't create a separate enterprise version of our product. Instead, all users - both commercial and non-commercial - use the same Netdata Agent, ensuring that all of our observability innovations are always open source.
To experience the full capabilities of the Netdata ecosystem, users need to combine the open-source components with our closed-source offerings. The complete product still remains free to use.
The closed-source components include:

Netdata UI: This is closed-source but free to use with the Netdata Agents and Netdata Cloud. It’s also publicly available via a CDN.
Netdata Cloud: A commercial product available both as an on-premises installation and as a SaaS solution, with a free community tier.

By balancing open-source and closed-source components, we ensure that all users have access to our innovations while sustaining our ability to grow and innovate as a company.
  

💰 What is your monetization strategy?
Netdata generates revenue through subscriptions to advanced features of Netdata Cloud and sales of on-premise and private versions of Netdata Cloud.
Click to see detailed answer ...
  
Netdata generates revenue from these activities:


Netdata Cloud Subscriptions
Direct funding for our project's vision comes from users subscribing to Netdata Cloud's advanced features.


Netdata Cloud On-Prem or Private
Purchasing the on-premises or private versions of Netdata Cloud supports our financial growth.


Our Open-Source Community and the free access to Netdata Cloud, contribute to Netdata in the following ways:


Netdata Cloud Community Use
The free usage of Netdata Cloud demonstrates its market relevance. While this doesn't generate revenue, it reinforces trust among new users and aids in securing appropriate project funding.


User Feedback
Feedback, especially issues and bug reports, is invaluable. It steers us towards a more resilient and efficient product. This, too, isn't a revenue source but is pivotal for our project's evolution.


Anonymous Telemetry Insights
Users who keep anonymous telemetry enabled, help us make data informed decisions on refining and enhancing Netdata. This isn't a revenue stream, but knowing which features are used and how, contributes in building a better product for everyone.


We don't monetize, directly or indirectly, users' or "device heuristics" data. Any data collected from community members is exclusively used for the purposes stated above.
Netdata grows financially when technology intensive organizations and operators need - due to regulatory or business requirements - the entire Netdata suite on-prem or private, bundled with top-tier support. It is a win-win case for all parties involved: these companies get a battle tested, robust and reliable solution, while the broader community that helps us build this product enjoys it at no cost.
  

📖 Documentation
Netdata's documentation is available at Netdata Learn.
This site also hosts a number of guides to help newer users better understand how
to collect metrics, troubleshoot via charts, export to external databases, and more.
🎉 Community





Netdata is an inclusive open-source project and community. Please read our Code of Conduct.
Join the Netdata community:

Chat with us and other community members on Discord.
Start a discussion on GitHub discussions.
Open a topic to our community forums.


Meet Up 🧑‍🤝‍🧑🧑‍🤝‍🧑🧑‍🤝‍🧑
The Netdata team and community members have regular online meetups.
You are welcome to join us!
Click here for the schedule.

You can also find Netdata on:
Twitter | YouTube | Reddit | LinkedIn | StackShare | Product Hunt | Repology | Facebook
🙏 Contribute



Contributions are essential to the success of open-source projects. In other words, we need your help to keep Netdata great!
What is a contribution? All the following are highly valuable to Netdata:


Let us know of the best practices you believe should be standardized
Netdata should out-of-the-box detect as many infrastructure issues as possible. By sharing your knowledge and experiences, you help us build a monitoring solution that has baked into it all the best-practices about infrastructure monitoring.


Let us know if Netdata is not perfect for your use case
We aim to support as many use cases as possible and your feedback can be invaluable. Open a GitHub issue, or start a GitHub discussion about it, to discuss how you want to use Netdata and what you need.
Although we can't implement everything imaginable, we try to prioritize development on use-cases that are common to our community, are in the same direction we want Netdata to evolve and are aligned with our roadmap.


Support other community members
Join our community on GitHub, Discord, and Reddit. Generally, Netdata is relatively easy to set up and configure, but still people may need a little push in the right direction to use it effectively. Supporting other members is a great contribution by itself!


Add or improve integrations you need
Integrations tend to be easier and simpler to develop. If you would like to contribute your code to Netdata, we suggest that you start with the integrations you need, which Netdata doesn’t currently support.


General information about contributions:

Check our Security Policy.
Found a bug? Open a GitHub issue.
Read our Contributing Guide, which contains all the information you need to contribute to Netdata, such as improving our documentation, engaging in the community, and developing new features. We've made it as frictionless as possible, but if you need help, just ping us on our community forums!

Package maintainers should read the guide on building Netdata from source for
instructions on building each Netdata component from the source and preparing a package.
License
The Netdata ecosystem consists of three key parts:


Netdata Agent: The heart of the Netdata ecosystem, the Netdata Agent is an open-source tool that must be installed on all systems monitored by Netdata. It offers a wide range of essential features, including data collection via various plugins, an embedded high-performance time-series database (dbengine), unsupervised anomaly detection powered by edge-trained machine learning, alerting and notifications, as well as query and scoring engines with associated APIs. Additionally, it supports exporting data to third-party monitoring systems, among other capabilities.
The Netdata Agent is released under the GPLv3+ license and redistributes several other open-source tools and libraries, which are listed in the Netdata Agent third-party licenses.


Netdata Cloud: A commercial, closed-source component, Netdata Cloud enhances the capabilities of the open-source Netdata Agent by providing horizontal scalability, centralized alert notification dispatch (including a mobile app), user management, role-based access control, and other enterprise-grade features. It is available both as a SaaS solution and for on-premises deployment, with a free-to-use community tier also offered.


Netdata UI: The Netdata UI is closed-source, and handles all visualization and dashboard functionalities related to metrics, logs and other collected data, as well as the central configuration and management of the Netdata ecosystem. It serves both the Netdata Agent and Netdata Cloud. The Netdata UI is distributed in binary form with the Netdata Agent and is publicly accessible via a CDN, licensed under the Netdata Cloud UI License 1 (NCUL1). It integrates third-party open-source components, detailed in the Netdata UI third-party licenses.


The binary installation packages provided by Netdata include the Netdata Agent and the Netdata UI. Since the Netdata Agent is open-source, it is frequently packaged by third parties (e.g., Linux Distributions) excluding the closed-source components (Netdata UI is not included). While their packages can still be useful in providing the necessary back-ends and the APIs of a fully functional monitoring solution, we recommend using the installation packages we provide to experience the full feature set of Netdata.

==========

Repository: zammad
URL: https://github.com/kavspvt2803/zammad
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Welcome to Zammad
Are you juggling countless customer inquiries across multiple channels?
Struggling to keep your support team on the same page?
Or spending more time managing your helpdesk than delivering exceptional support to your customers?
Zammad is your Swiss Army knife - a web-based, open-source helpdesk and customer support platform
packed with features to streamline customer communication across channels like email, chat, telephone and social media.
The Software
The Zammad software is and will stay open source. It is licensed under the GNU AGPLv3.
The source code is available on GitHub and owned by
the Zammad Foundation, which is independent of commercial
providers such as Zammad GmbH.
The Company - Zammad GmbH
The development of Zammad is carried out by the amazing team of people
at Zammad GmbH in collaboration with the community.
We love to create open source software for you. If you want to ensure the Zammad software
has a bright and sustainable future, consider becoming a Zammad customer!

Are you tired of complex setup, configuration, backup and update tasks? Let us handle this stuff for you! 🚀
The easiest and often most cost-effective way to operate Zammad is our cloud service.
Give it a try with a free trial instance!

Status

Toolchain: 


Docker container images: 

Helm chart for Kubernetes: 

Download DEB/RPM: 

License: 

Further Information

Installing & Getting Started
Screenshots
Developer Manual
REST API
For reporting security vulnerabilities, please see our security policy.
Contributing

Thanks! ❤️ ❤️ ❤️
Your Zammad Team

==========

Repository: rustdesk
URL: https://github.com/kavspvt2803/rustdesk
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Build •
  Docker •
  Structure •
  Snapshot
  [Українська] | [česky] | [中文] | [Magyar] | [Español] | [فارسی] | [Français] | [Deutsch] | [Polski] | [Indonesian] | [Suomi] | [മലയാളം] | [日本語] | [Nederlands] | [Italiano] | [Русский] | [Português (Brasil)] | [Esperanto] | [한국어] | [العربي] | [Tiếng Việt] | [Dansk] | [Ελληνικά] | [Türkçe] | [Norsk]
We need your help to translate this README, RustDesk UI and RustDesk Doc to your native language

CautionMisuse Disclaimer: 
The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.

Chat with us: Discord | Twitter | Reddit

Yet another remote desktop software, written in Rust. Works out of the box, no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, set up your own, or write your own rendezvous/relay server.

RustDesk welcomes contribution from everyone. See CONTRIBUTING.md for help getting started.
FAQ
BINARY DOWNLOAD
NIGHTLY BUILD


Dependencies
Desktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our CI for building Flutter version.
Please download Sciter dynamic library yourself.
Windows |
Linux |
macOS
Raw steps to build


Prepare your Rust development env and C++ build env


Install vcpkg, and set VCPKG_ROOT env variable correctly

Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static
Linux/macOS: vcpkg install libvpx libyuv opus aom



run cargo run


Build
How to build on Linux
Ubuntu 18 (Debian 10)
sudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \
        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \
        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev
openSUSE Tumbleweed
sudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel
Fedora 28 (CentOS 8)
sudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel
Arch (Manjaro)
sudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire
Install vcpkg
git clone https://github.com/microsoft/vcpkg
cd vcpkg
git checkout 2023.04.15
cd ..
vcpkg/bootstrap-vcpkg.sh
export VCPKG_ROOT=$HOME/vcpkg
vcpkg/vcpkg install libvpx libyuv opus aom
Fix libvpx (For Fedora)
cd vcpkg/buildtrees/libvpx/src
cd *
./configure
sed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile
sed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile
make
cp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/
cd
Build
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
git clone --recurse-submodules https://github.com/rustdesk/rustdesk
cd rustdesk
mkdir -p target/debug
wget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so
mv libsciter-gtk.so target/debug
VCPKG_ROOT=$HOME/vcpkg cargo run
How to build with Docker
Begin by cloning the repository and building the Docker container:
git clone https://github.com/rustdesk/rustdesk
cd rustdesk
git submodule update --init --recursive
docker build -t "rustdesk-builder" .
Then, each time you need to build the application, run the following command:
docker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID="$(id -u)" -e PGID="$(id -g)" rustdesk-builder
Note that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the <OPTIONAL-ARGS> position. For instance, if you wanted to build an optimized release version, you would run the command above followed by --release. The resulting executable will be available in the target folder on your system, and can be run with:
target/debug/rustdesk
Or, if you're running a release executable:
target/release/rustdesk
Please ensure that you are running these commands from the root of the RustDesk repository, otherwise the application might not be able to find the required resources. Also note that other cargo subcommands such as install or run are not currently supported via this method as they would install or run the program inside the container instead of the host.
File Structure

libs/hbb_common: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions
libs/scrap: screen capture
libs/enigo: platform specific keyboard/mouse control
libs/clipboard: file copy and paste implementation for Windows, Linux, macOS.
src/ui: obsolete Sciter UI (deprecated)
src/server: audio/clipboard/input/video services, and network connections
src/client.rs: start a peer connection
src/rendezvous_mediator.rs: Communicate with rustdesk-server, wait for remote direct (TCP hole punching) or relayed connection
src/platform: platform specific code
flutter: Flutter code for desktop and mobile
flutter/web/js: JavaScript for Flutter web client

Screenshots

==========

Repository: gitea
URL: https://github.com/kavspvt2803/gitea
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Gitea










繁體中文 | 简体中文
Purpose
The goal of this project is to make the easiest, fastest, and most
painless way of setting up a self-hosted Git service.
As Gitea is written in Go, it works across all the platforms and
architectures that are supported by Go, including Linux, macOS, and
Windows on x86, amd64, ARM and PowerPC architectures.
This project has been
forked from
Gogs since November of 2016, but a lot has changed.
For online demonstrations, you can visit demo.gitea.com.
For accessing free Gitea service (with a limited number of repositories), you can visit gitea.com.
To quickly deploy your own dedicated Gitea instance on Gitea Cloud, you can start a free trial at cloud.gitea.com.
Documentation
You can find comprehensive documentation on our official documentation website.
It includes installation, administration, usage, development, contributing guides, and more to help you get started and explore all features effectively.
If you have any suggestions or would like to contribute to it, you can visit the documentation repository
Building
From the root of the source tree, run:
TAGS="bindata" make build

or if SQLite support is required:
TAGS="bindata sqlite sqlite_unlock_notify" make build

The build target is split into two sub-targets:

make backend which requires Go Stable, the required version is defined in go.mod.
make frontend which requires Node.js LTS or greater.

Internet connectivity is required to download the go and npm modules. When building from the official source tarballs which include pre-built frontend files, the frontend target will not be triggered, making it possible to build without Node.js.
More info: https://docs.gitea.com/installation/install-from-source
Using
After building, a binary file named gitea will be generated in the root of the source tree by default. To run it, use:
./gitea web

NoteIf you're interested in using our APIs, we have experimental support with documentation.

Contributing
Expected workflow is: Fork -> Patch -> Push -> Pull Request
Note

YOU MUST READ THE CONTRIBUTORS GUIDE BEFORE STARTING TO WORK ON A PULL REQUEST.
If you have found a vulnerability in the project, please write privately to security@gitea.io. Thanks!


Translating

Translations are done through Crowdin. If you want to translate to a new language ask one of the managers in the Crowdin project to add a new language there.
You can also just create an issue for adding a language or ask on discord on the #translation channel. If you need context or find some translation issues, you can leave a comment on the string or ask on Discord. For general translation questions there is a section in the docs. Currently a bit empty but we hope to fill it as questions pop up.
Get more information from documentation.
Official and Third-Party Projects
We provide an official go-sdk, a CLI tool called tea and an action runner for Gitea Action.
We maintain a list of Gitea-related projects at gitea/awesome-gitea, where you can discover more third-party projects, including SDKs, plugins, themes, and more.
Communication

If you have questions that are not covered by the documentation, you can get in contact with us on our Discord server or create a post in the discourse forum.
Authors

Maintainers
Contributors
Translators

Backers
Thank you to all our backers! 🙏 [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










FAQ
How do you pronounce Gitea?
Gitea is pronounced /ɡɪ’ti:/ as in "gi-tea" with a hard g.
Why is this not hosted on a Gitea instance?
We're working on it.
Where can I find the security patches?
In the release log or the change log, search for the keyword SECURITY to find the security patches.
License
This project is licensed under the MIT License.
See the LICENSE file
for the full license text.
Further information

Looking for an overview of the interface? Check it out!
Login/Register Page


User Dashboard




User Profile

Explore



Repository







Repository Issue


Repository Pull Requests




Repository Actions


Repository Activity




Organization

==========

Repository: serenity
URL: https://github.com/kavspvt2803/serenity
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
SerenityOS
Graphical Unix-like operating system for 64-bit x86, Arm, and RISC-V computers.



FAQ | Documentation | Build Instructions
About
SerenityOS is a love letter to '90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.
Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.
You can watch videos of the system being developed on YouTube:

Andreas Kling's channel
Linus Groh's channel
kleines Filmröllchen's channel

Screenshot

Features

Modern 64-bit kernel with pre-emptive multi-threading
Browser with JavaScript, WebAssembly, and more (check the spec compliance for JS, CSS, and Wasm)
Security features (hardware protections, limited userland capabilities, W^X memory, pledge & unveil, (K)ASLR, OOM-resistance, web-content isolation, state-of-the-art TLS algorithms, ...)
System services (WindowServer, LoginServer, AudioServer, WebServer, RequestServer, CrashServer, ...) and modern IPC
Good POSIX compatibility (LibC, Shell, syscalls, signals, pseudoterminals, filesystem notifications, standard Unix utilities, ...)
POSIX-like virtual file systems (/proc, /dev, /sys, /tmp, ...) and ext2 file system
Network stack and applications with support for IPv4, TCP, UDP; DNS, HTTP, Gemini, IMAP, NTP
Profiling, debugging and other development tools (Kernel-supported profiling, CrashReporter, interactive GUI playground, HexEditor, HackStudio IDE for C++ and more)
Libraries for everything from cryptography to OpenGL, audio, JavaScript, GUI, playing chess, ...
Support for many common and uncommon file formats (PNG, JPEG, GIF, MP3, WAV, FLAC, ZIP, TAR, PDF, QOI, Gemini, ...)
Unified style and design philosophy, flexible theming system, custom (bitmap and vector) fonts
Games (Solitaire, Minesweeper, 2048, chess, Conway's Game of Life, ...) and demos (CatDog, Starfield, Eyes, mandelbrot set, WidgetGallery, ...)
Every-day GUI programs and utilities (Spreadsheet with JavaScript, TextEditor, Terminal, PixelPaint, various multimedia viewers and players, Mail, Assistant, Calculator, ...)

... and all of the above are right in this repository, no extra dependencies, built from-scratch by us :^)
Additionally, there are over three hundred ports of popular open-source software, including games, compilers, Unix tools, multimedia apps and more.
How do I read the documentation?
Man pages are available online at man.serenityos.org. These pages are generated from the Markdown source files in Base/usr/share/man and updated automatically.
When running SerenityOS you can use man for the terminal interface, or help for the GUI.
Code-related documentation can be found in the documentation folder.
How do I build and run this?
See the SerenityOS build instructions or the Ladybird build instructions.
The build system supports a cross-compilation build of SerenityOS from Linux, macOS, Windows (with WSL2) and many other *Nixes.
The default build system commands will launch a QEMU instance running the OS with hardware or software virtualization
enabled as supported.
Ladybird runs on the same platforms that can be the host for a cross build of SerenityOS and on SerenityOS itself.
Get in touch and participate!
Join our Discord server: SerenityOS Discord
Before opening an issue, please see the issue policy.
A general guide for contributing can be found in CONTRIBUTING.md.
Authors

Andreas Kling - awesomekling 
Robin Burchell - rburchell
Conrad Pankoff - deoxxa
Sergey Bugaev - bugaevc
Liav A - supercomputer7
Linus Groh - linusg 
Ali Mohammad Pur - alimpfard
Shannon Booth - shannonbooth
Hüseyin ASLITÜRK - asliturk
Matthew Olsson - mattco98
Nico Weber - nico
Brian Gianforcaro - bgianfo
Ben Wiederhake - BenWiederhake
Tom - tomuta
Paul Scharnofske - asynts
Itamar Shenhar - itamar8910
Luke Wilde - Lubrsi
Brendan Coles - bcoles
Andrew Kaster - ADKaster 
thankyouverycool - thankyouverycool
Idan Horowitz - IdanHo
Gunnar Beutner - gunnarbeutner
Tim Flynn - trflynn89
Jean-Baptiste Boric - boricj
Stephan Unverwerth - sunverwerth
Max Wipfli - MaxWipfli
Daniel Bertalan - BertalanD
Jelle Raaijmakers - GMTA
Sam Atkins - AtkinsSJ 
Tobias Christiansen - TobyAsE
Lenny Maiorani - ldm5180
sin-ack - sin-ack
Jesse Buhagiar - Quaker762
Peter Elliott - Petelliott
Karol Kosek - krkk
Mustafa Quraish - mustafaquraish
David Tuin - davidot
Leon Albrecht - Hendiadyoin1
Tim Schumacher - timschumi
Marcus Nilsson - metmo
Gegga Thor - Xexxa 
kleines Filmröllchen - kleinesfilmroellchen 
Kenneth Myhra - kennethmyhra
Maciej - sppmacd
Sahan Fernando - ccapitalK
Benjamin Maxwell - MacDue
Dennis Esternon - djwisdom 
frhun - frhun
networkException - networkException 
Brandon Jordan - electrikmilk
Lucas Chollet - LucasChollet
Timon Kruiper - FireFox317
Martin Falisse - martinfalisse
Gregory Bertilson - Zaggy1024
Erik Wouters - EWouters
Rodrigo Tobar - rtobar
Alexander Kalenik - kalenikaliaksandr
Tim Ledbetter - tcl3
Steffen T. Larssen - stelar7
Andi Gallo - axgallo
Simon Wanner - skyrising
FalseHonesty - FalseHonesty
Bastiaan van der Plaat - bplaat
Dan Klishch - DanShaders
Julian Offenhäuser - janso3
Sönke Holz - spholz
implicitfield - implicitfield

And many more! See here for a full contributor list. The people listed above have landed more than 100 commits in the project. :^)
License
SerenityOS is licensed under a 2-clause BSD license.

==========

Repository: microcks
URL: https://github.com/kavspvt2803/microcks
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Microcks - Kubernetes native tool for API Mocking & Testing
Microcks is a platform for turning your API and microservices assets - OpenAPI specs, AsyncAPI specs, gRPC protobuf, GraphQL schema, Postman collections, SoapUI projects - into live mocks in seconds.
It also reuses these assets for running compliance and non-regression tests against your API implementation. We provide integrations with Jenkins, GitHub Actions, Tekton and many others through a simple CLI.
Getting Started

Documentation
Microcks Community and community meeting

To get involved with our community, please make sure you are familiar with the project's Code of Conduct.
Build Status
The current development version is 1.12.0-SNAPSHOT on branch 1.12.x.

Sonarcloud Quality metrics







Fossa license and security scans



Signature, Provenance, SBOM

OpenSSF best practices


Versions
Here are the naming conventions we're using for current releases, ongoing development maintenance activities.



Status
Version
Branch
Container images tags




Stable
1.11.2
master
1.11.2, latest


Dev
1.12.0-SNAPSHOT
1.12.x
nightly


Maintenance
1.11.3-SNAPSHOT
1.11.x
maintenance



How to build Microcks
The build instructions are available in the contribution guide.
Thanks to community!

==========

Repository: netron
URL: https://github.com/kavspvt2803/netron
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Netron is a viewer for neural network, deep learning and machine learning models.
Netron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy.
Netron has experimental support for TorchScript, torch.export, ExecuTorch, TensorFlow, OpenVINO, RKNN, ncnn, MNN, PaddlePaddle, GGUF and scikit-learn.

Install
macOS: Download the .dmg file or run brew install --cask netron
Linux: Download the .AppImage file or run snap install netron
Windows: Download the .exe installer or run winget install -s winget netron
Browser: Start the browser version.
Python: pip install netron, then run netron [FILE] or netron.start('[FILE]').
Models
Sample model files to download or open using the browser version:

ONNX: squeezenet [open]
TorchScript: traced_online_pred_layer [open]
TensorFlow Lite: yamnet [open]
TensorFlow: chessbot [open]
Keras: mobilenet [open]
Core ML: exermote [open]
Darknet: yolo [open]

==========

Repository: dragonfly
URL: https://github.com/kavspvt2803/dragonfly
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Dragonfly










Provide efficient, stable and secure file distribution and image acceleration
based on p2p technology to be the best practice and
standard solution in cloud native architectures.
Introduction
Dragonfly is an open source P2P-based file distribution and
image acceleration system. It is hosted by the
Cloud Native Computing Foundation (CNCF) as
an Incubating Level Project.
Its goal is to tackle all distribution problems in cloud native architectures.
Currently Dragonfly focuses on being:

Simple: Well-defined user-facing API (HTTP), non-invasive to all container engines;
Efficient: Seed peer support, P2P based file distribution to save enterprise bandwidth;
Intelligent: Host-level speed limit, intelligent flow control due to host detection;
Secure: Block transmission encryption, HTTPS connection support.

Architecture

Manager: Maintain the relationship between each P2P cluster, dynamic configuration management and RBAC.
It also includes a front-end console, which is convenient for users to visually operate the cluster.
Scheduler: Select the optimal download parent peer for the download peer. Exceptions control Dfdaemon's back-to-source.
Seed Peer: Dfdaemon turns on the Seed Peer mode can be used as
a back-to-source download peer in a P2P cluster,
which is the root peer for download in the entire cluster.
Peer: Deploy with dfdaemon, based on the C/S architecture, it provides the dfget command download tool,
and the dfget daemon running daemon to provide task download capabilities.
Documentation
You can find the full documentation on the d7y.io.
Security
Security Audit
A third party security audit was performed by Trail of Bits,
you can see the full report here.
Community
Join the conversation and help the community.

Slack Channel: #dragonfly on CNCF Slack
Discussion Group: dragonfly-discuss@googlegroups.com
Developer Group: dragonfly-developers@googlegroups.com
Github Discussions: Dragonfly Discussion Forum
Twitter: @dragonfly_oss
DingTalk: 22880028764

Contributing
You should check out our
CONTRIBUTING and develop the project together.
Code of Conduct
Please refer to our Code of Conduct.

==========

Repository: jumpserver
URL: https://github.com/kavspvt2803/jumpserver
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
An open-source PAM tool (Bastion Host)





English · 中文(简体) · 中文(繁體) · 日本語 · Português (Brasil) · Español · Русский


What is JumpServer?
JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.





Quickstart
Prepare a clean Linux Server ( 64 bit, >= 4c8g )
curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash
Access JumpServer in your browser at http://your-jumpserver-ip/

Username: admin
Password: ChangeMe


Screenshots
    
       
    



















Components
JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.



Project
Status
Description




Lina

JumpServer Web UI


Luna

JumpServer Web Terminal


KoKo

JumpServer Character Protocol Connector


Lion

JumpServer Graphical Protocol Connector


Chen

JumpServer Web DB


Tinker

JumpServer Remote Application Connector (Windows)


Panda

JumpServer EE Remote Application Connector (Linux)


Razor

JumpServer EE RDP Proxy Connector


Magnus

JumpServer EE Database Proxy Connector


Nec

JumpServer EE VNC Proxy Connector


Facelive

JumpServer EE Facial Recognition



Contributing
Welcome to submit PR to contribute. Please refer to CONTRIBUTING.md for guidelines.
License
Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.
Licensed under The GNU General Public License version 3 (GPLv3) (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at
https://www.gnu.org/licenses/gpl-3.0.html
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an " AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

==========

Repository: nuclei
URL: https://github.com/kavspvt2803/nuclei
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
English •
中文 •
Korean •
Indonesia •
Spanish •
日本語
Portuguese



  

  

  




Nuclei is a modern, high-performance vulnerability scanner that leverages simple YAML-based templates. It empowers you to design custom vulnerability detection scenarios that mimic real-world conditions, leading to zero false positives.

Simple YAML format for creating and customizing vulnerability templates.
Contributed by thousands of security professionals to tackle trending vulnerabilities.
Reduce false positives by simulating real-world steps to verify a vulnerability.
Ultra-fast parallel scan processing and request clustering.
Integrate into CI/CD pipelines for vulnerability detection and regression testing.
Supports multiple protocols like TCP, DNS, HTTP, SSL, WHOIS JavaScript, Code and more.
Integrate with Jira, Splunk, GitHub, Elastic, GitLab.



Table of Contents

Get Started

1. Nuclei CLI
2. Pro and Enterprise Editions


Documentation

Command Line Flags
Single target scan
Scanning multiple targets
Network scan
Scanning with your custom template
Connect Nuclei to ProjectDiscovery_


Nuclei Templates, Community and Rewards 💎
Our Mission
Contributors ❤
License



Get Started
1. Nuclei CLI
Install Nuclei on your machine. Get started by following the installation guide here. Additionally, We provide a free cloud tier and comes with a generous monthly free limits:

Store and visualize your vulnerability findings
Write and manage your nuclei templates
Access latest nuclei templates
Discover and store your targets

Important



This project is in active development. Expect breaking changes with releases. Review the release changelog before updating.




This project is primarily built to be used as a standalone CLI tool. Running nuclei as a service may pose security risks. It's recommended to use with caution and additional security measures.





2. Pro and Enterprise Editions
For security teams and enterprises, we provide a cloud-hosted service built on top of Nuclei OSS, fine-tuned to help you continuously run vulnerability scans at scale with your team and existing workflows:

50x faster scans
Large scale scanning with high accuracy
Integrations with cloud services (AWS, GCP, Azure, CloudFlare, Fastly, Terraform, Kubernetes)
Jira, Slack, Linear, APIs and Webhooks
Executive and compliance reporting
Plus: Real-time scanning, SAML SSO, SOC 2 compliant platform (with EU and US hosting options), shared team workspaces, and more
We're constantly adding new features!
Ideal for: Pentesters, security teams, and enterprises

Sign up to Pro or Talk to our team if you have large organization and complex requirements.


Documentation
Browse the full Nuclei documentation here. If you’re new to Nuclei, check out our foundational Youtube series.

 


Installation
nuclei requires go1.22 to install successfully. Run the following command to get the repo:
go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
To learn more about installing nuclei, see https://docs.projectdiscovery.io/tools/nuclei/install.
Command Line Flags
To display all the flags for the tool:
nuclei -h

Expand full help flags
Nuclei is a fast, template based vulnerability scanner focusing
on extensive configurability, massive extensibility and ease of use.

Usage:
  ./nuclei [flags]

Flags:
TARGET:
   -u, -target string[]          target URLs/hosts to scan
   -l, -list string              path to file containing a list of target URLs/hosts to scan (one per line)
   -eh, -exclude-hosts string[]  hosts to exclude to scan from the input list (ip, cidr, hostname)
   -resume string                resume scan using resume.cfg (clustering will be disabled)
   -sa, -scan-all-ips            scan all the IP's associated with dns record
   -iv, -ip-version string[]     IP version to scan of hostname (4,6) - (default 4)

TARGET-FORMAT:
   -im, -input-mode string        mode of input file (list, burp, jsonl, yaml, openapi, swagger) (default "list")
   -ro, -required-only            use only required fields in input format when generating requests
   -sfv, -skip-format-validation  skip format validation (like missing vars) when parsing input file

TEMPLATES:
   -nt, -new-templates                    run only new templates added in latest nuclei-templates release
   -ntv, -new-templates-version string[]  run new templates added in specific version
   -as, -automatic-scan                   automatic web scan using wappalyzer technology detection to tags mapping
   -t, -templates string[]                list of template or template directory to run (comma-separated, file)
   -turl, -template-url string[]          template url or list containing template urls to run (comma-separated, file)
   -ai, -prompt string                    generate and run template using ai prompt
   -w, -workflows string[]                list of workflow or workflow directory to run (comma-separated, file)
   -wurl, -workflow-url string[]          workflow url or list containing workflow urls to run (comma-separated, file)
   -validate                              validate the passed templates to nuclei
   -nss, -no-strict-syntax                disable strict syntax check on templates
   -td, -template-display                 displays the templates content
   -tl                                    list all available templates
   -tgl                                   list all available tags
   -sign                                  signs the templates with the private key defined in NUCLEI_SIGNATURE_PRIVATE_KEY env variable
   -code                                  enable loading code protocol-based templates
   -dut, -disable-unsigned-templates      disable running unsigned templates or templates with mismatched signature
   -esc, -enable-self-contained           enable loading self-contained templates
   -egm, -enable-global-matchers          enable loading global matchers templates
   -file                                  enable loading file templates

FILTERING:
   -a, -author string[]               templates to run based on authors (comma-separated, file)
   -tags string[]                     templates to run based on tags (comma-separated, file)
   -etags, -exclude-tags string[]     templates to exclude based on tags (comma-separated, file)
   -itags, -include-tags string[]     tags to be executed even if they are excluded either by default or configuration
   -id, -template-id string[]         templates to run based on template ids (comma-separated, file, allow-wildcard)
   -eid, -exclude-id string[]         templates to exclude based on template ids (comma-separated, file)
   -it, -include-templates string[]   path to template file or directory to be executed even if they are excluded either by default or configuration
   -et, -exclude-templates string[]   path to template file or directory to exclude (comma-separated, file)
   -em, -exclude-matchers string[]    template matchers to exclude in result
   -s, -severity value[]              templates to run based on severity. Possible values: info, low, medium, high, critical, unknown
   -es, -exclude-severity value[]     templates to exclude based on severity. Possible values: info, low, medium, high, critical, unknown
   -pt, -type value[]                 templates to run based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -ept, -exclude-type value[]        templates to exclude based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript
   -tc, -template-condition string[]  templates to run based on expression condition

OUTPUT:
   -o, -output string            output file to write found issues/vulnerabilities
   -sresp, -store-resp           store all request/response passed through nuclei to output directory
   -srd, -store-resp-dir string  store all request/response passed through nuclei to custom directory (default "output")
   -silent                       display findings only
   -nc, -no-color                disable output content coloring (ANSI escape codes)
   -j, -jsonl                    write output in JSONL(ines) format
   -irr, -include-rr -omit-raw   include request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only) [DEPRECATED use -omit-raw] (default true)
   -or, -omit-raw                omit request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only)
   -ot, -omit-template           omit encoded template in the JSON, JSONL output
   -nm, -no-meta                 disable printing result metadata in cli output
   -ts, -timestamp               enables printing timestamp in cli output
   -rdb, -report-db string       nuclei reporting database (always use this to persist report data)
   -ms, -matcher-status          display match failure status
   -me, -markdown-export string  directory to export results in markdown format
   -se, -sarif-export string     file to export results in SARIF format
   -je, -json-export string      file to export results in JSON format
   -jle, -jsonl-export string    file to export results in JSONL(ine) format
   -rd, -redact string[]         redact given list of keys from query parameter, request header and body

CONFIGURATIONS:
   -config string                        path to the nuclei configuration file
   -tp, -profile string                  template profile config file to run
   -tpl, -profile-list                   list community template profiles
   -fr, -follow-redirects                enable following redirects for http templates
   -fhr, -follow-host-redirects          follow redirects on the same host
   -mr, -max-redirects int               max number of redirects to follow for http templates (default 10)
   -dr, -disable-redirects               disable redirects for http templates
   -rc, -report-config string            nuclei reporting module configuration file
   -H, -header string[]                  custom header/cookie to include in all http request in header:value format (cli, file)
   -V, -var value                        custom vars in key=value format
   -r, -resolvers string                 file containing resolver list for nuclei
   -sr, -system-resolvers                use system DNS resolving as error fallback
   -dc, -disable-clustering              disable clustering of requests
   -passive                              enable passive HTTP response processing mode
   -fh2, -force-http2                    force http2 connection on requests
   -ev, -env-vars                        enable environment variables to be used in template
   -cc, -client-cert string              client certificate file (PEM-encoded) used for authenticating against scanned hosts
   -ck, -client-key string               client key file (PEM-encoded) used for authenticating against scanned hosts
   -ca, -client-ca string                client certificate authority file (PEM-encoded) used for authenticating against scanned hosts
   -sml, -show-match-line                show match lines for file templates, works with extractors only
   -ztls                                 use ztls library with autofallback to standard one for tls13 [Deprecated] autofallback to ztls is enabled by default
   -sni string                           tls sni hostname to use (default: input domain name)
   -dka, -dialer-keep-alive value        keep-alive duration for network requests.
   -lfa, -allow-local-file-access        allows file (payload) access anywhere on the system
   -lna, -restrict-local-network-access  blocks connections to the local / private network
   -i, -interface string                 network interface to use for network scan
   -at, -attack-type string              type of payload combinations to perform (batteringram,pitchfork,clusterbomb)
   -sip, -source-ip string               source ip address to use for network scan
   -rsr, -response-size-read int         max response size to read in bytes
   -rss, -response-size-save int         max response size to read in bytes (default 1048576)
   -reset                                reset removes all nuclei configuration and data files (including nuclei-templates)
   -tlsi, -tls-impersonate               enable experimental client hello (ja3) tls randomization
   -hae, -http-api-endpoint string       experimental http api endpoint

INTERACTSH:
   -iserver, -interactsh-server string  interactsh server url for self-hosted instance (default: oast.pro,oast.live,oast.site,oast.online,oast.fun,oast.me)
   -itoken, -interactsh-token string    authentication token for self-hosted interactsh server
   -interactions-cache-size int         number of requests to keep in the interactions cache (default 5000)
   -interactions-eviction int           number of seconds to wait before evicting requests from cache (default 60)
   -interactions-poll-duration int      number of seconds to wait before each interaction poll request (default 5)
   -interactions-cooldown-period int    extra time for interaction polling before exiting (default 5)
   -ni, -no-interactsh                  disable interactsh server for OAST testing, exclude OAST based templates

FUZZING:
   -ft, -fuzzing-type string           overrides fuzzing type set in template (replace, prefix, postfix, infix)
   -fm, -fuzzing-mode string           overrides fuzzing mode set in template (multiple, single)
   -fuzz                               enable loading fuzzing templates (Deprecated: use -dast instead)
   -dast                               enable / run dast (fuzz) nuclei templates
   -dts, -dast-server                  enable dast server mode (live fuzzing)
   -dtr, -dast-report                  write dast scan report to file
   -dtst, -dast-server-token string    dast server token (optional)
   -dtsa, -dast-server-address string  dast server address (default "localhost:9055")
   -dfp, -display-fuzz-points          display fuzz points in the output for debugging
   -fuzz-param-frequency int           frequency of uninteresting parameters for fuzzing before skipping (default 10)
   -fa, -fuzz-aggression string        fuzzing aggression level controls payload count for fuzz (low, medium, high) (default "low")
   -cs, -fuzz-scope string[]           in scope url regex to be followed by fuzzer
   -cos, -fuzz-out-scope string[]      out of scope url regex to be excluded by fuzzer

UNCOVER:
   -uc, -uncover                  enable uncover engine
   -uq, -uncover-query string[]   uncover search query
   -ue, -uncover-engine string[]  uncover search engine (shodan,censys,fofa,shodan-idb,quake,hunter,zoomeye,netlas,criminalip,publicwww,hunterhow,google) (default shodan)
   -uf, -uncover-field string     uncover fields to return (ip,port,host) (default "ip:port")
   -ul, -uncover-limit int        uncover results to return (default 100)
   -ur, -uncover-ratelimit int    override ratelimit of engines with unknown ratelimit (default 60 req/min) (default 60)

RATE-LIMIT:
   -rl, -rate-limit int               maximum number of requests to send per second (default 150)
   -rld, -rate-limit-duration value   maximum number of requests to send per second (default 1s)
   -rlm, -rate-limit-minute int       maximum number of requests to send per minute (DEPRECATED)
   -bs, -bulk-size int                maximum number of hosts to be analyzed in parallel per template (default 25)
   -c, -concurrency int               maximum number of templates to be executed in parallel (default 25)
   -hbs, -headless-bulk-size int      maximum number of headless hosts to be analyzed in parallel per template (default 10)
   -headc, -headless-concurrency int  maximum number of headless templates to be executed in parallel (default 10)
   -jsc, -js-concurrency int          maximum number of javascript runtimes to be executed in parallel (default 120)
   -pc, -payload-concurrency int      max payload concurrency for each template (default 25)
   -prc, -probe-concurrency int       http probe concurrency with httpx (default 50)

OPTIMIZATIONS:
   -timeout int                     time to wait in seconds before timeout (default 10)
   -retries int                     number of times to retry a failed request (default 1)
   -ldp, -leave-default-ports       leave default HTTP/HTTPS ports (eg. host:80,host:443)
   -mhe, -max-host-error int        max errors for a host before skipping from scan (default 30)
   -te, -track-error string[]       adds given error to max-host-error watchlist (standard, file)
   -nmhe, -no-mhe                   disable skipping host from scan based on errors
   -project                         use a project folder to avoid sending same request multiple times
   -project-path string             set a specific project path (default "/tmp")
   -spm, -stop-at-first-match       stop processing HTTP requests after the first match (may break template/workflow logic)
   -stream                          stream mode - start elaborating without sorting the input
   -ss, -scan-strategy value        strategy to use while scanning(auto/host-spray/template-spray) (default auto)
   -irt, -input-read-timeout value  timeout on input read (default 3m0s)
   -nh, -no-httpx                   disable httpx probing for non-url input
   -no-stdin                        disable stdin processing

HEADLESS:
   -headless                        enable templates that require headless browser support (root user on Linux will disable sandbox)
   -page-timeout int                seconds to wait for each page in headless mode (default 20)
   -sb, -show-browser               show the browser on the screen when running templates with headless mode
   -ho, -headless-options string[]  start headless chrome with additional options
   -sc, -system-chrome              use local installed Chrome browser instead of nuclei installed
   -lha, -list-headless-action      list available headless actions

DEBUG:
   -debug                     show all requests and responses
   -dreq, -debug-req          show all sent requests
   -dresp, -debug-resp        show all received responses
   -p, -proxy string[]        list of http/socks5 proxy to use (comma separated or file input)
   -pi, -proxy-internal       proxy all internal requests
   -ldf, -list-dsl-function   list all supported DSL function signatures
   -tlog, -trace-log string   file to write sent requests trace log
   -elog, -error-log string   file to write sent requests error log
   -version                   show nuclei version
   -hm, -hang-monitor         enable nuclei hang monitoring
   -v, -verbose               show verbose output
   -profile-mem string        generate memory (heap) profile & trace files
   -vv                        display templates loaded for scan
   -svd, -show-var-dump       show variables dump for debugging
   -vdl, -var-dump-limit int  limit the number of characters displayed in var dump (default 255)
   -ep, -enable-pprof         enable pprof debugging server
   -tv, -templates-version    shows the version of the installed nuclei-templates
   -hc, -health-check         run diagnostic check up

UPDATE:
   -up, -update                      update nuclei engine to the latest released version
   -ut, -update-templates            update nuclei-templates to latest released version
   -ud, -update-template-dir string  custom directory to install / update nuclei-templates
   -duc, -disable-update-check       disable automatic nuclei/templates update check

STATISTICS:
   -stats                    display statistics about the running scan
   -sj, -stats-json          display statistics in JSONL(ines) format
   -si, -stats-interval int  number of seconds to wait between showing a statistics update (default 5)
   -mp, -metrics-port int    port to expose nuclei metrics on (default 9092)
   -hps, -http-stats         enable http status capturing (experimental)

CLOUD:
   -auth                           configure projectdiscovery cloud (pdcp) api key (default true)
   -tid, -team-id string           upload scan results to given team id (optional) (default "none")
   -cup, -cloud-upload             upload scan results to pdcp dashboard [DEPRECATED use -dashboard]
   -sid, -scan-id string           upload scan results to existing scan id (optional)
   -sname, -scan-name string       scan name to set (optional)
   -pd, -dashboard                 upload / view nuclei results in projectdiscovery cloud (pdcp) UI dashboard
   -pdu, -dashboard-upload string  upload / view nuclei results file (jsonl) in projectdiscovery cloud (pdcp) UI dashboard

AUTHENTICATION:
   -sf, -secret-file string[]  path to config file containing secrets for nuclei authenticated scan
   -ps, -prefetch-secrets      prefetch secrets from the secrets file


EXAMPLES:
Run nuclei on single host:
	$ nuclei -target example.com

Run nuclei with specific template directories:
	$ nuclei -target example.com -t http/cves/ -t ssl

Run nuclei against a list of hosts:
	$ nuclei -list hosts.txt

Run nuclei with a JSON output:
	$ nuclei -target example.com -json-export output.json

Run nuclei with sorted Markdown outputs (with environment variables):
	$ MARKDOWN_EXPORT_SORT_MODE=template nuclei -target example.com -markdown-export nuclei_report/

Additional documentation is available at: https://docs.nuclei.sh/getting-started/running

Additional documentation is available at: docs.nuclei.sh/getting-started/running

Single target scan
To perform a quick scan on web-application:
nuclei -target https://example.com
Scanning multiple targets
Nuclei can handle bulk scanning by providing a list of targets. You can use a file containing multiple URLs.
nuclei -list urls.txt
Network scan
This will scan the entire subnet for network-related issues, such as open ports or misconfigured services.
nuclei -target 192.168.1.0/24
Scanning with your custom template
To write and use your own template, create a .yaml file with specific rules, then use it as follows.
nuclei -u https://example.com -t /path/to/your-template.yaml
Connect Nuclei to ProjectDiscovery
You can run the scans on your machine and upload the results to the cloud platform for further analysis and remediation.
nuclei -target https://example.com -dashboard
NoteThis feature is absolutely free and does not require any subscription. For a detailed guide, refer to the documentation.



Nuclei Templates, Community and Rewards 💎
Nuclei templates are based on the concepts of YAML based template files that define how the requests will be sent and processed. This allows easy extensibility capabilities to nuclei. The templates are written in YAML which specifies a simple human-readable format to quickly define the execution process.
Try it online with our free AI powered Nuclei Templates Editor by clicking here.
Nuclei Templates offer a streamlined way to identify and communicate vulnerabilities, combining essential details like severity ratings and detection methods. This open-source, community-developed tool accelerates threat response and is widely recognized in the cybersecurity world. Nuclei templates are actively contributed by thousands of security researchers globally. We run two programs for our contributors: Pioneers and 💎 bounties.



Examples
Visit our documentation for use cases and ideas.



Use case
Nuclei template




Detect known CVEs
CVE-2021-44228 (Log4Shell)


Identify Out-of-Band vulnerabilities
Blind SQL Injection via OOB


SQL Injection detection
Generic SQL Injection


Cross-Site Scripting (XSS)
Reflected XSS Detection


Default or weak passwords
Default Credentials Check


Secret files or data exposure
Sensitive File Disclosure


Identify open redirects
Open Redirect Detection


Detect subdomain takeovers
Subdomain Takeover Templates


Security misconfigurations
Unprotected Jenkins Console


Weak SSL/TLS configurations
SSL Certificate Expiry


Misconfigured cloud services
Open S3 Bucket Detection


Remote code execution vulnerabilities
RCE Detection Templates


Directory traversal attacks
Path Traversal Detection


File inclusion vulnerabilities
Local/Remote File Inclusion





Our Mission
Traditional vulnerability scanners were built decades ago. They are closed-source, incredibly slow, and vendor-driven. Today's attackers are mass exploiting newly released CVEs across the internet within days, unlike the years it used to take. This shift requires a completely different approach to tackling trending exploits on the internet.
We built Nuclei to solve this challenge. We made the entire scanning engine framework open and customizable—allowing the global security community to collaborate and tackle the trending attack vectors and vulnerabilities on the internet. Nuclei is now used and contributed by Fortune 500 enterprises, government agencies, universities.
You can participate by contributing to our code, templates library, or joining our team.


Contributors ❤️
Thanks to all the amazing community contributors for sending PRs and keeping this project updated. ❤️









































































































































































nuclei is distributed under MIT License

==========

Repository: wasmCloud
URL: https://github.com/kavspvt2803/wasmCloud
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
wasmCloud is an open source Cloud Native Computing Foundation (CNCF) project that enables teams to build, manage, and scale polyglot Wasm apps across any cloud, K8s, or edge.
wasmCloud offers faster development cycles with reusable, polyglot components and centrally maintainable apps, allowing platform teams to manage thousands of diverse applications. It integrates seamlessly with existing stacks like Kubernetes and cloud providers, while providing portability across different operating systems and architectures without new builds. With custom capabilities, scale-to-zero, fault-tolerant features, and deployment across clouds, wasmCloud enables secure, reliable, and scalable applications without vendor lock-in.
Getting Started
Installation
Install the wasmCloud Shell (wash) using the installation guide.
Walkthrough
If you're new to the wasmCloud ecosystem, a great place to start is the getting started walkthrough.
Quickstart
The following commands launch wasmCloud in a local development environment and deploy a simple "hello world" WebAssembly component, written in Rust, Go, TypeScript, or Python.
wash new component helloworld
wash dev --work-dir helloworld
curl localhost:8000
Features

Declarative WebAssembly Orchestration
Seamless Distributed Networking
Vendorless Application Components
Completely OTEL Observable
Defense-In-Depth Security By Default

Examples
👟 Runnable examples
Want to get something running quickly? Check out the examples directory of this repository. Examples are organized by programming language so you can easily find samples in your language of choice.
🗺️ Roadmap and Vision
wasmCloud is a community-led project and plans quarterly roadmaps in community meetings. Please check out the latest roadmap for more information, and the wasmCloud Roadmap project to see the status of new features, improvements, bug fixes, and documentation.
Releases
The latest release and changelog can be found on the releases page.
🧑‍💻 Contributing
Want to get involved? For more information on how to contribute and our contributor guidelines, check out the contributing readme.

🌇 Community Resources
Community Meetings
We host weekly community meetings at 1pm EST on Wednesdays. These community meetings are livestreamed to our Twitter account and to YouTube. You can find the agenda and notes for each meeting in the community section of our website. If you're interested in joining in on the call to demo or take part in the discussion, we have a Zoom link on our community calendar.
Slack
We host our own community slack for all community members to join and talk about WebAssembly, wasmCloud, or just general cloud native technology. For those of you who are already on the CNCF Slack, we also have our own channel at #wasmcloud.

📚 Reference Documentation
wasmCloud uses some terminology you might not be familiar with. Check out the platform overview section of our docs for a deeper dive.

RPC Framework (wRPC)
wasmCloud uses wRPC, Component-native transport-agnostic RPC protocol and framework based on WebAssembly Interface Types (WIT) to enable seamless communication among the host runtime, components, and providers. wRPC is a Bytecode Alliance hosted project.

Wasm-native Orchestration & Declarative Deployments
The wasmCloud Application Deployment Manager wadm is a Wasm-native orchestrator for managing and scaling declarative wasmCloud applications. Applications are defined using the Open Application Model format.

Language Support & SDKs
wasmCloud is compatible with any language that supports the WebAssembly Component Model. We provide first-party examples in Rust, Go, Python, and TypeScript. If your language isn't listed yet, let us know with the language support form.
Capability Provider SDK
wasmCloud provides the following SDKs for creating capability providers; native executable host plugins for extending wasmCloud with custom implementations or custom capabilities:

Rust provider-sdk, with a custom template provider built for getting started quickly
Golang provider-sdk-go, with a custom template provider built for getting started quickly


We are a Cloud Native Computing Foundation Incubating project.

==========

Repository: generator
URL: https://github.com/kavspvt2803/generator
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
MyBatis Generator (MBG)








This is a code generator for MyBatis.
This library will generate code for use with MyBatis. It will introspect a database table (or many tables) and will generate artifacts that can be used to access the table(s). This lessens the initial nuisance of setting up objects and configuration files to interact with database tables. MBG seeks to make a major impact on the large percentage of database operations that are simple CRUD (Create, Retrieve, Update, Delete).
MBG can generate code in multiple styles (or "runtimes"). MBG can generate code for Java based projects, or for Kotlin based projects.
Eclipse Update Site (Direct)
https://jeffgbutler.github.io/mybatis-generator-update-site/
Eclipse Update Site (Marketplace)
https://marketplace.eclipse.org/content/mybatis-generator

==========

Repository: dokku
URL: https://github.com/kavspvt2803/dokku
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Dokku








Docker powered mini-Heroku. The smallest PaaS implementation you've ever seen.
Sponsors
Become a sponsor and get your logo on our README on GitHub with a link to your site. [Become a sponsor]



















Backers
Support us with a monthly donation and help us continue our activities. [Become a backer]






























Requirements
A fresh VM running any of the following operating systems:

Ubuntu 20.04 / 22.04 / 24.04 (amd64/arm64) - Any currently supported release
Debian 11+ (amd64/arm64)

An SSH keypair that can be used for application deployment. If this exists before installation, it will be automatically imported into dokku.
Otherwise, you will need to import the keypair manually after installation using dokku ssh-keys:add.
Installation
To install the latest stable release, run the following commands as a user who has access to sudo:
wget -NP . https://dokku.com/install/v0.35.18/bootstrap.sh
sudo DOKKU_TAG=v0.35.18 bash bootstrap.sh
You can then proceed to configure your server domain (via dokku domains:set-global) and user access (via dokku ssh-keys:add) to complete the installation.
If you wish for a more unattended installation method, see these docs.
Upgrade
View the docs for upgrading from an older version of Dokku.
Documentation
Full documentation - including advanced installation docs - are available online at https://dokku.com/docs/getting-started/installation/.
Support
You can use GitHub Issues, check Troubleshooting in the documentation, or join us on Gliderlabs Slack in the #dokku channel.
Contribution
After checking GitHub Issues, the Troubleshooting Guide or having a chat with us on Gliderlabs Slack in the #dokku channel, feel free to fork and create a Pull Request.
While we may not merge your PR as is, they serve to start conversations and improve the general Dokku experience for all users.
License
MIT License © Jeff Lindsay

==========

Repository: cml
URL: https://github.com/kavspvt2803/cml
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
What is CML? Continuous Machine Learning (CML) is an open-source CLI tool
for implementing continuous integration & delivery (CI/CD) with a focus on
MLOps. Use it to automate development workflows — including machine
provisioning, model training and evaluation, comparing ML experiments across
project history, and monitoring changing datasets.
CML can help train and evaluate models — and then generate a visual report with
results and metrics — automatically on every pull request.
 An
example report for a
neural style transfer model.
CML principles:

GitFlow for data
science. Use GitLab or GitHub to manage ML experiments, track who trained ML
models or modified data and when. Codify data and models with
DVC instead of pushing to a Git repo.
Auto reports for ML experiments. Auto-generate reports with metrics and
plots in each Git pull request. Rigorous engineering practices help your team
make informed, data-driven decisions.
No additional services. Build your own ML platform using GitLab,
Bitbucket, or GitHub. Optionally, use
cloud storage as well as either
self-hosted or cloud runners (such as AWS EC2 or Azure). No databases,
services or complex setup needed.

❓ Need help? Just want to chat about continuous integration for ML?
Visit our Discord channel!
⏯️ Check out our
YouTube video series
for hands-on MLOps tutorials using CML!
Table of Contents

Setup (GitLab, GitHub, Bitbucket)
Usage
Getting started (tutorial)
Using CML with DVC
Advanced Setup (Self-hosted, local package)
Example projects

Setup
You'll need a GitLab, GitHub, or Bitbucket account to begin. Users may wish to
familiarize themselves with Github Actions
or
GitLab CI/CD.
Here, will discuss the GitHub use case.
GitLab
Please see our docs on
CML with GitLab CI/CD
and in particular the
personal access token
requirement.
Bitbucket
Please see our docs on
CML with Bitbucket Cloud.
GitHub
The key file in any CML project is .github/workflows/cml.yaml:
name: your-workflow-name
on: [push]
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    # container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      # - uses: actions/setup-node@v3
      #   with:
      #     node-version: '16'
      # - uses: actions/setup-python@v4
      #   with:
      #     python-version: '3.x'
      - uses: iterative/setup-cml@v1
      - name: Train model
        run: |
          # Your ML workflow goes here
          pip install -r requirements.txt
          python train.py
      - name: Write CML report
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Post reports as comments in GitHub PRs
          cat results.txt >> report.md
          cml comment create report.md
Usage
We helpfully provide CML and other useful libraries pre-installed on our
custom Docker images.
In the above example, uncommenting the field
container: ghcr.io/iterative/cml:0-dvc2-base1) will make the runner pull the
CML Docker image. The image already has NodeJS, Python 3, DVC and CML set up on
an Ubuntu LTS base for convenience.
CML Functions
CML provides a number of functions to help package the outputs of ML workflows
(including numeric data and visualizations about model performance) into a CML
report.
Below is a table of CML functions for writing markdown reports and delivering
those reports to your CI system.



Function
Description
Example Inputs




cml runner launch
Launch a runner locally or hosted by a cloud provider
See Arguments


cml comment create
Return CML report as a comment in your GitLab/GitHub workflow
<path to report> --head-sha <sha>


cml check create
Return CML report as a check in GitHub
<path to report> --head-sha <sha>


cml pr create
Commit the given files to a new branch and create a pull request
<path>...


cml tensorboard connect
Return a link to a Tensorboard.dev page
--logdir <path to logs> --title <experiment title> --md



CML Reports
The cml comment create command can be used to post reports. CML reports are
written in markdown (GitHub,
GitLab, or
Bitbucket
flavors). That means they can contain images, tables, formatted text, HTML
blocks, code snippets and more — really, what you put in a CML report is up to
you. Some examples:
🗒️ Text Write to your report using whatever method you prefer.
For example, copy the contents of a text file containing the results of ML model
training:
cat results.txt >> report.md
🖼️ Images Display images using the markdown or HTML. Note that
if an image is an output of your ML workflow (i.e., it is produced by your
workflow), it can be uploaded and included automaticlly to your CML report. For
example, if graph.png is output by python train.py, run:
echo "![](./graph.png)" >> report.md
cml comment create report.md
Getting Started

Fork our
example project repository.


⚠️ Note that if you are using GitLab,
you will need to create a Personal Access Token
for this example to work.



⚠️ The following steps can all be done in the GitHub browser interface.
However, to follow along with the commands, we recommend cloning your fork to
your local workstation:

git clone https://github.com/<your-username>/example_cml

To create a CML workflow, copy the following into a new file,
.github/workflows/cml.yaml:

name: model-training
on: [push]
jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - uses: iterative/setup-cml@v1
      - name: Train model
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          pip install -r requirements.txt
          python train.py

          cat metrics.txt >> report.md
          echo "![](./plot.png)" >> report.md
          cml comment create report.md


In your text editor of choice, edit line 16 of train.py to depth = 5.


Commit and push the changes:


git checkout -b experiment
git add . && git commit -m "modify forest depth"
git push origin experiment

In GitHub, open up a pull request to compare the experiment branch to
main.


Shortly, you should see a comment from github-actions appear in the pull
request with your CML report. This is a result of the cml send-comment
function in your workflow.

This is the outline of the CML workflow:

you push changes to your GitHub repository,
the workflow in your .github/workflows/cml.yaml file gets run, and
a report is generated and posted to GitHub.

CML functions let you display relevant results from the workflow — such as model
performance metrics and visualizations — in GitHub checks and comments. What
kind of workflow you want to run, and want to put in your CML report, is up to
you.
Using CML with DVC
In many ML projects, data isn't stored in a Git repository, but needs to be
downloaded from external sources. DVC is a common way to
bring data to your CML runner. DVC also lets you visualize how metrics differ
between commits to make reports like this:

The .github/workflows/cml.yaml file used to create this report is:
name: model-training
on: [push]
jobs:
  run:
    runs-on: ubuntu-latest
    container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
      - name: Train model
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # Install requirements
          pip install -r requirements.txt

          # Pull data & run-cache from S3 and reproduce pipeline
          dvc pull data --run-cache
          dvc repro

          # Report metrics
          echo "## Metrics" >> report.md
          git fetch --prune
          dvc metrics diff main --show-md >> report.md

          # Publish confusion matrix diff
          echo "## Plots" >> report.md
          echo "### Class confusions" >> report.md
          dvc plots diff --target classes.csv --template confusion -x actual -y predicted --show-vega main > vega.json
          vl2png vega.json -s 1.5 > confusion_plot.png
          echo "![](./confusion_plot.png)" >> report.md

          # Publish regularization function diff
          echo "### Effects of regularization" >> report.md
          dvc plots diff --target estimators.csv -x Regularization --show-vega main > vega.json
          vl2png vega.json -s 1.5 > plot.png
          echo "![](./plot.png)" >> report.md

          cml comment create report.md

⚠️ If you're using DVC with cloud storage, take note of environment
variables for your storage format.

Configuring Cloud Storage Providers
There are many
supported could storage providers.
Here are a few examples for some of the most frequently used providers:


  S3 and S3-compatible storage (Minio, DigitalOcean Spaces, IBM Cloud Object Storage...)
  
# Github
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}

👉 AWS_SESSION_TOKEN is optional.


👉 AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY can also be used
by cml runner to launch EC2 instances. See [Environment Variables].




  Azure
  
env:
  AZURE_STORAGE_CONNECTION_STRING:
    ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}
  AZURE_STORAGE_CONTAINER_NAME: ${{ secrets.AZURE_STORAGE_CONTAINER_NAME }}



  Aliyun
  
env:
  OSS_BUCKET: ${{ secrets.OSS_BUCKET }}
  OSS_ACCESS_KEY_ID: ${{ secrets.OSS_ACCESS_KEY_ID }}
  OSS_ACCESS_KEY_SECRET: ${{ secrets.OSS_ACCESS_KEY_SECRET }}
  OSS_ENDPOINT: ${{ secrets.OSS_ENDPOINT }}



  Google Storage
  

⚠️ Normally, GOOGLE_APPLICATION_CREDENTIALS is the path of the
json file containing the credentials. However in the action this secret
variable is the contents of the file. Copy the json contents and add it
as a secret.

env:
  GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}



  Google Drive
  

⚠️ After configuring your
Google Drive credentials
you will find a json file at
your_project_path/.dvc/tmp/gdrive-user-credentials.json. Copy its contents
and add it as a secret variable.

env:
  GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}

Advanced Setup
Self-hosted (On-premise or Cloud) Runners
GitHub Actions are run on GitHub-hosted runners by default. However, there are
many great reasons to use your own runners: to take advantage of GPUs,
orchestrate your team's shared computing resources, or train in the cloud.

☝️ Tip! Check out the
official GitHub documentation
to get started setting up your own self-hosted runner.

Allocating Cloud Compute Resources with CML
When a workflow requires computational resources (such as GPUs), CML can
automatically allocate cloud instances using cml runner. You can spin up
instances on AWS, Azure, GCP, or Kubernetes.
For example, the following workflow deploys a g4dn.xlarge instance on AWS EC2
and trains a model on the instance. After the job runs, the instance
automatically shuts down.
You might notice that this workflow is quite similar to the
basic use case above. The only addition is cml runner and a few
environment variables for passing your cloud service credentials to the
workflow.
Note that cml runner will also automatically restart your jobs (whether from a
GitHub Actions 35-day workflow timeout
or a
AWS EC2 spot instance interruption).
name: Train-in-the-cloud
on: [push]
jobs:
  deploy-runner:
    runs-on: ubuntu-latest
    steps:
      - uses: iterative/setup-cml@v1
      - uses: actions/checkout@v3
      - name: Deploy runner on EC2
        env:
          REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          cml runner launch \
            --cloud=aws \
            --cloud-region=us-west \
            --cloud-type=g4dn.xlarge \
            --labels=cml-gpu
  train-model:
    needs: deploy-runner
    runs-on: [self-hosted, cml-gpu]
    timeout-minutes: 50400 # 35 days
    container:
      image: ghcr.io/iterative/cml:0-dvc2-base1-gpu
      options: --gpus all
    steps:
      - uses: actions/checkout@v3
      - name: Train model
        env:
          REPO_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
        run: |
          pip install -r requirements.txt
          python train.py

          cat metrics.txt > report.md
          cml comment create report.md
In the workflow above, the deploy-runner step launches an EC2 g4dn.xlarge
instance in the us-west region. The model-training step then runs on the
newly-launched instance. See [Environment Variables] below for details on the
secrets required.

🎉 Note that jobs can use any Docker container! To use functions such
as cml send-comment from a job, the only requirement is to
have CML installed.

Docker Images
The CML Docker image (ghcr.io/iterative/cml or iterativeai/cml) comes loaded
with Python, CUDA, git, node and other essentials for full-stack data
science. Different versions of these essentials are available from different
image tags. The tag convention is {CML_VER}-dvc{DVC_VER}-base{BASE_VER}{-gpu}:



{BASE_VER}
Software included (-gpu)




0
Ubuntu 18.04, Python 2.7 (CUDA 10.1, CuDNN 7)


1
Ubuntu 20.04, Python 3.8 (CUDA 11.2, CuDNN 8)



For example, iterativeai/cml:0-dvc2-base1-gpu, or
ghcr.io/iterative/cml:0-dvc2-base1.
Arguments
The cml runner launch function accepts the following arguments:
  --labels                                  One or more user-defined labels for
                                            this runner (delimited with commas)
                                                       [string] [default: "cml"]
  --idle-timeout                            Time to wait for jobs before
                                            shutting down (e.g. "5min"). Use
                                            "never" to disable
                                                 [string] [default: "5 minutes"]
  --name                                    Name displayed in the repository
                                            once registered
                                                    [string] [default: cml-{ID}]
  --no-retry                                Do not restart workflow terminated
                                            due to instance disposal or GitHub
                                            Actions timeout            [boolean]
  --single                                  Exit after running a single job
                                                                       [boolean]
  --reuse                                   Don't launch a new runner if an
                                            existing one has the same name or
                                            overlapping labels         [boolean]
  --reuse-idle                              Creates a new runner only if the
                                            matching labels don't exist or are
                                            already busy               [boolean]
  --docker-volumes                          Docker volumes, only supported in
                                            GitLab         [array] [default: []]
  --cloud                                   Cloud to deploy the runner
                         [string] [choices: "aws", "azure", "gcp", "kubernetes"]
  --cloud-region                            Region where the instance is
                                            deployed. Choices: [us-east,
                                            us-west, eu-west, eu-north]. Also
                                            accepts native cloud regions
                                                   [string] [default: "us-west"]
  --cloud-type                              Instance type. Choices: [m, l, xl].
                                            Also supports native types like i.e.
                                            t2.micro                    [string]
  --cloud-permission-set                    Specifies the instance profile in
                                            AWS or instance service account in
                                            GCP           [string] [default: ""]
  --cloud-metadata                          Key Value pairs to associate
                                            cml-runner instance on the provider
                                            i.e. tags/labels "key=value"
                                                           [array] [default: []]
  --cloud-gpu                               GPU type. Choices: k80, v100, or
                                            native types e.g. nvidia-tesla-t4
                                                                        [string]
  --cloud-hdd-size                          HDD size in GB              [number]
  --cloud-ssh-private                       Custom private RSA SSH key. If not
                                            provided an automatically generated
                                            throwaway key will be used  [string]
  --cloud-spot                              Request a spot instance    [boolean]
  --cloud-spot-price                        Maximum spot instance bidding price
                                            in USD. Defaults to the current spot
                                            bidding price [number] [default: -1]
  --cloud-startup-script                    Run the provided Base64-encoded
                                            Linux shell script during the
                                            instance initialization     [string]
  --cloud-aws-security-group                Specifies the security group in AWS
                                                          [string] [default: ""]
  --cloud-aws-subnet,                       Specifies the subnet to use within
  --cloud-aws-subnet-id                     AWS           [string] [default: ""]


Environment Variables

⚠️ You will need to
create a personal access token (PAT)
with repository read/write access and workflow privileges. In the example
workflow, this token is stored as PERSONAL_ACCESS_TOKEN.

ℹ️ If using the --cloud option, you will also need to
provide access credentials of your cloud compute resources as secrets. In the
above example, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (with privileges
to create & destroy EC2 instances) are required.
For AWS, the same credentials can also be used for
configuring cloud storage.
Proxy support
CML support proxy via known environment variables http_proxy and
https_proxy.
On-premise (Local) Runners
This means using on-premise machines as self-hosted runners. The
cml runner launch function is used to set up a local self-hosted runner. On a
local machine or on-premise GPU cluster,
install CML as a package and then run:
cml runner launch \
  --repo=$your_project_repository_url \
  --token=$PERSONAL_ACCESS_TOKEN \
  --labels="local,runner" \
  --idle-timeout=180
The machine will listen for workflows from your project repository.
Local Package
In the examples above, CML is installed by the setup-cml action, or comes
pre-installed in a custom Docker image pulled by a CI runner. You can also
install CML as a package:
npm install --location=global @dvcorg/cml
You can use cml without node by downloading the correct standalone binary for
your system from the asset section of the
releases.
You may need to install additional dependencies to use DVC plots and Vega-Lite
CLI commands:
sudo apt-get install -y libcairo2-dev libpango1.0-dev libjpeg-dev libgif-dev \
                        librsvg2-dev libfontconfig-dev
npm install -g vega-cli vega-lite
CML and Vega-Lite package installation require the NodeJS package manager
(npm) which ships with NodeJS. Installation instructions are below.
Install NodeJS

GitHub: This is probably not necessary when using GitHub's default
containers or one of CML's Docker containers. Self-hosted runners may need to
use a set up action to install NodeJS:

uses: actions/setup-node@v3
  with:
    node-version: '16'

GitLab: Requires direct installation.

curl -sL https://deb.nodesource.com/setup_16.x | bash
apt-get update
apt-get install -y nodejs
See Also
These are some example projects using CML.

Basic CML project
CML with DVC to pull data
CML with Tensorboard
CML with a small EC2 instance
🔑
CML with EC2 GPU
🔑

🔑 needs a PAT.
⚠️ Maintenance ⚠️

~2023-07 Nvidia has dropped container CUDA images with
10.x/cudnn7
and 11.2.1,
CML images will be updated accrodingly

==========

Repository: onedev
URL: https://github.com/kavspvt2803/onedev
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
**NOTE: ** We develop OneDev at code.onedev.io for sake of dogfooding. Please submit issues and pull requests there



Git Server with CI/CD, Kanban, and Packages

Get Started



🔎 Out-of-box code search and navigation
Language aware symbol search and navigation in any commit.
Click symbol to show occurrences in current file.
Fast code search with regular expression.
Try It

📦 Renovate integration to update project dependencies
Integrate with Renovate to update project dependencies via pull requests.
Merge pull requests automatically when approved by required reviewers, or pass required tests.
Tutorial

🚦 Annotate code with coverage and problems
Code will be annotated with coverage info and problems found in
CI/CD pipeline, to facilitate code review.
Demo

💬 Code discussion anywhere anytime
Select any code or diff to start discussion. Suggest and apply changes.
Discussions stay with code to help code understanding.
See It In Action

🔒 Versatile code protection rules
Set rule to require review or CI/CD verification when certain users touch certain
files in certain branches.
Tutorial

📋 Automated Kanban to keep team organized
Move tasks manually in Kanban, or define rules to move them automatically
when related work is committed/tested/released/deployed.
See It In Action

🛠 Customizable and flexible issue workflow
Custom issue states and fields. Manual or automatic state transition rules.
Issue links to sync operations and states. Confidential issues in public projects.
Tutorial

📨 Service desk to link emails with issues
Use issues as ticket system to support customers via email, without requiring
them to register accounts. Assign different support contacts for different
projects or customers.
Tutorial

⏰ Time tracking and reporting
Track estimated/spent time on tasks. Aggregate time from subtasks automatically.
Generate time sheets for work statistics and billing.
Tutorial

💡 CI/CD as code without writing code
An intuitive GUI to create CI/CD jobs. Template for typical frameworks.
Typed parameters. Matrix jobs. CI/CD logic reuses. Cache management.
Tutorial

🚀 Versatile CI/CD executors from simple to scale
Run CI/CD out-of-box in container or on bare metal. Run massive jobs concurrently
with Kubernetes or agents.
Example1
Example2

🛠 Tools to debug CI/CD jobs
Command to pause job execution. Web terminal to check job execution environment.
Run job locally against uncommitted changes.
Tutorial1
Tutorial2

📦 Built-in package registries
Built-in registry to manage binary packages. Link packages with
CI/CD jobs.
Tutorial

🧩 Deep integration and information cross-reference
Transit issue state via commit, CI/CD, or pull request.
Show fixing builds of issue. Query fixed issues or code changes between build/package versions.
Example1
Example2

🌲 Project tree for easy maintenance
Use tree to organize projects clearly and efficiently.
Define common settings in parent project and inherit in child projects.
See It In Action

🐒 Smart query that can be saved and subscribed
Powerful and intuitive query for everything. Save query for quick access. Subscribe to
query to get notified of interesting events.
Try It

🎛️ Dashboard for teams and users
Arrange gadgets in custom dashboard to get important information
at a glance. Share dashboard with users or groups, or make it public
for everyone.
See It In Action

👯 Effortless high availability and scalability
Easy cluster setup. Replicate projects across different servers
for high availability, or distribute projects for horizontal scalability.
More Info

🛸 Command palette for quick access
Use cmd/ctrl-k to bring up command palette from anywhere.
Search anything and jump to it without digging through menus.
Try It

📈 SLOC trend by language
Inspects git history of main branch to calculate trend of
source lines of code by language efficiently.
See It In Action

🕊️ Fast, lightweight, and reliable
Crafted with resource usage and performance in mind. Get all features above with a 1 core 2G mem box
for medium-sized projects. Intensively used for more than 5 years, with battle-proven reliability.
Performance Comparison

==========

Repository: tracecat
URL: https://github.com/kavspvt2803/tracecat
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Tracecat is a modern, open source workflow automation platform built for security and IT engineers. Simple YAML-based templates for integrations with a no-code UI for workflows.
Executed using Temporal for scale and reliability.
We're on a mission to make security and IT automation more accessible through response-as-code. What Sigma rules did for detection, YARA for malware research, and Nuclei did for vulnerabilities, Tracecat is doing for response automation.
Getting Started
ImportantTracecat is in active development. Expect breaking changes with releases. Review the release changelog before updating.

Run Tracecat locally
Deploy a local Tracecat stack using Docker Compose. View full instructions here.
# Download Tracecat
git clone https://github.com/TracecatHQ/tracecat.git

# Generate .env file
./env.sh

# Run Tracecat
docker compose up -d
Go to http://localhost to access the UI. Sign-up with your email and password (min 12 characters). The first user to sign-up and login will be the superadmin for the instance. The API docs is accessible at http://localhost/api/docs.
Run Tracecat on AWS Fargate
For advanced users: deploy a production-ready Tracecat stack on AWS Fargate using Terraform. View full instructions here.
# Download Terraform files
git clone https://github.com/TracecatHQ/tracecat.git
cd tracecat/deployments/aws

# Create and add encryption keys to AWS Secrets Manager
./scripts/create-aws-secrets.sh

# Run Terraform to deploy Tracecat
terraform init
terraform apply
Run Tracecat on Kubernetes
Coming soon.
Community
Have questions? Feedback? New integration ideas? Come hang out with us in the Tracecat Community Discord.
Tracecat Registry

Tracecat Registry is a collection of integration and response-as-code templates.
Response actions are organized into MITRE D3FEND categories (detect, isolate, evict, restore, harden, model) and Tracecat's own ontology of capabilities (e.g. list_alerts, list_cases, list_users). Template inputs (e.g. start_time, end_time) are normalized to fit the Open Cyber Security Schema (OCSF) ontology where possible.
The future of response automation should be self-serve, where teams rapidly link common capabilities (e.g. list_alerts -> enrich_ip_address -> block_ip_address) into workflows.
Examples
Visit our documentation on Tracecat Registry for use cases and ideas.
Or check out existing open source templates in our repo.
Open Source vs Enterprise
This repo is available under the AGPL-3.0 license with the exception of the ee directory. The ee directory contains paid enterprise features requiring a Tracecat Enterprise license.
Tracecat Enteprise builds on top of Tracecat OSS, optimized for mixed ETL and network workloads at enterprise scale.
Powered by serverless workflow execution (AWS Lambda and Knative) and S3-compatible object storage.
If you are interested in Tracecat's Enterprise self-hosted or managed Cloud offering, check out our website or book a meeting with us.
Security
SSO, audit logs, and IaaC deployments (Terraform, Kubernetes / Helm) will always be free and available. We're working on a comprehensive list of Tracecat's threat model, security features, and hardening recommendations. For immediate answers to these questions, please reach to us on Discord.
Please report any security issues to security@tracecat.com and include tracecat in the subject line.
Contributors
Thank you all our amazing contributors for contributing code, integrations, and support. Open source is only possible because of you. ❤️






Tracecat is distributed under AGPL-3.0

==========

Repository: osquery
URL: https://github.com/kavspvt2803/osquery
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
osquery




osquery is a SQL powered operating system instrumentation, monitoring, and analytics framework.

Available for Linux, macOS, and Windows.

Information and resources

Homepage: osquery.io
Downloads: osquery.io/downloads
Documentation: ReadTheDocs
Stack Overflow: Stack Overflow questions
Table Schema: osquery.io/schema
Query Packs: osquery.io/packs
Slack: Browse the archives or Join the conversation
Build Status:   
CII Best Practices: 

What is osquery?
osquery exposes an operating system as a high-performance relational database.  This allows you to
write SQL-based queries to explore operating system data.  With osquery, SQL tables represent
abstract concepts such as running processes, loaded kernel modules, open network connections,
browser plugins, hardware events or file hashes.
SQL tables are implemented via a simple plugin and extensions API. A variety of tables already exist
and more are being written: https://osquery.io/schema. To best
understand the expressiveness that is afforded to you by osquery, consider the following SQL
queries:
List the users:
SELECT * FROM users;
Check the processes that have a deleted executable:
SELECT * FROM processes WHERE on_disk = 0;
Get the process name, port, and PID, for processes listening on all interfaces:
SELECT DISTINCT processes.name, listening_ports.port, processes.pid
  FROM listening_ports JOIN processes USING (pid)
  WHERE listening_ports.address = '0.0.0.0';
Find every macOS LaunchDaemon that launches an executable and keeps it running:
SELECT name, program || program_arguments AS executable
  FROM launchd
  WHERE (run_at_load = 1 AND keep_alive = 1)
  AND (program != '' OR program_arguments != '');
Check for ARP anomalies from the host's perspective:
SELECT address, mac, COUNT(mac) AS mac_count
  FROM arp_cache GROUP BY mac
  HAVING count(mac) > 1;
Alternatively, you could also use a SQL sub-query to accomplish the same result:
SELECT address, mac, mac_count
  FROM
    (SELECT address, mac, COUNT(mac) AS mac_count FROM arp_cache GROUP BY mac)
  WHERE mac_count > 1;
These queries can be:

performed on an ad-hoc basis to explore operating system state using the
osqueryi shell
executed via a scheduler
to monitor operating system state across a set of hosts
launched from custom applications using osquery Thrift APIs

Download & Install
To download the latest stable builds and for repository information
and installation instructions visit
https://osquery.io/downloads.
We use a simple numbered versioning scheme X.Y.Z, where X is a major version, Y is a minor, and Z is a patch.
We plan minor releases roughly every two months. These releases are tracked on our Milestones page. A patch release is used when there are unforeseen bugs with our minor release and we need to quickly patch.
A rare 'revision' release might be used if we need to change build configurations.
Major, minor, and patch releases are tagged on GitHub and can be viewed on the Releases page.
We open a new Release Checklist issue when we prepare a minor release. If you are interested in the status of a release, please find the corresponding checklist issue, and note that the issue will be marked closed when we are finished the checklist.
We consider a release 'in testing' during the period of hosting new downloads on our website and adding them to our hosted repositories.
We will mark the release as 'stable' on GitHub when enough testing has occurred, this usually takes two weeks.
Build from source
Building osquery from source is encouraged! Check out our build
guide. Also
check out our contributing guide and join the
community on Slack.
Osquery fleet managers
There are many osquery fleet managers out there. The osquery project does not endorse, recommend, or test these. They are provided as a starting point



Project
License




Fleet
Open Core


Kolide
Commercial


OSCTRL
Open Source


Zentral
Open Source



License
By contributing to osquery you agree that your contributions will be
licensed as defined on the LICENSE file.
Vulnerabilities
We keep track of security announcements in our tagged version release
notes on GitHub. We aggregate these into SECURITY.md
too.
Learn more
The osquery documentation is available
online. Documentation for older
releases can be found by version number, as
well.
If you're interested in learning more about osquery read the launch
blog
post
for background on the project, visit the users
guide.
Development and usage discussion is happening in the osquery Slack, grab an invite
here!

==========

Repository: bitcore
URL: https://github.com/kavspvt2803/bitcore
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Bitcore Monorepo








Infrastructure to build Bitcoin and blockchain-based applications for the next generation of financial technology.
Applications

Bitcore Node - A standardized API to interact with multiple blockchain networks
Bitcore Wallet - A command-line based wallet client
Bitcore Wallet Client - A client for the wallet service
Bitcore Wallet Service - A multisig HD service for wallets
Bitpay Wallet - An easy-to-use, multiplatform, multisignature, secure bitcoin wallet
Insight - A blockchain explorer web user interface

Libraries

Bitcore Lib - A powerful JavaScript library for Bitcoin
Bitcore Lib Cash - A powerful JavaScript library for Bitcoin Cash
Bitcore Lib Doge - A powerful JavaScript library for Dogecoin
Bitcore Lib Litecoin - A powerful JavaScript library for Litecoin
Bitcore Mnemonic - Implements mnemonic code for generating deterministic keys
Bitcore P2P - The peer-to-peer networking protocol for Bitcoin
Bitcore P2P Cash - The peer-to-peer networking protocol for Bitcoin Cash
Bitcore P2P Doge DEPRECATED1 - The peer-to-peer networking protocol for Dogecoin
Crypto Wallet Core - A coin-agnostic wallet library for creating transactions, signing, and address derivation

Extras

Bitcore Build - A helper to add tasks to gulp
Bitcore Client - A helper to create a wallet using the bitcore-v8 infrastructure

Contributing
See CONTRIBUTING.md on the main bitcore repo for information about how to contribute.
License
Code released under the MIT license.
Copyright 2013-2023 BitPay, Inc. Bitcore is a trademark maintained by BitPay, Inc.
Footnotes


The Bitcore P2P Doge library is no longer maintained as all the core functionality is contained in Bitcore P2P ↩

==========

Repository: caldera
URL: https://github.com/kavspvt2803/caldera
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
MITRE Caldera™
MITRE Caldera™ is a cyber security platform designed to easily automate adversary emulation, assist manual red-teams, and automate incident response.
It is built on the MITRE ATT&CK™ framework and is an active research project at MITRE.
The framework consists of two components:

The core system. This is the framework code, consisting of what is available in this repository. Included is
an asynchronous command-and-control (C2) server with a REST API and a web interface.
Plugins. These repositories expand the core framework capabilities and providing additional functionality. Examples include agents, reporting, collections of TTPs and more.

Resources & Socials

📜 Documentation, training, and use-cases
🎬 Tutorial Videos
✍️ Caldera's blog
🌐 Homepage

User Survey
It is always incredibly helpful for our team to hear from users about their Caldera use cases and the value that Caldera provides for their learning, research, or cyber security work. If you or your team uses Caldera significantly, we would greatly appreciate hearing from you.
📋 Survey - https://forms.office.com/g/ByBWxYTf8e
Plugins
⭐ Create your own plugin! Plugin generator: Skeleton ⭐
Default
These plugins are supported and maintained by the Caldera team.

Access (red team initial access tools and techniques)
Atomic (Atomic Red Team project TTPs)
Builder (dynamically compile payloads)
Caldera for OT (ICS/OT capabilities for Caldera)
Compass (ATT&CK visualizations)
Debrief (operations insights)
Emu (CTID emulation plans)
Fieldmanual (documentation)
GameBoard (visualize joint red and blue operations)
Human (create simulated noise on an endpoint)
Magma (VueJS UI for Caldera v5)
Manx (shell functionality and reverse shell payloads)
Response (incident response)
Sandcat (default agent)
SSL (enable https for caldera)
Stockpile (technique and profile storehouse)
Training (certification and training course)

More
These plugins are ready to use but are not included by default and are not maintained by the Caldera team.

Arsenal (MITRE ATLAS techniques and profiles)
BountyHunter (The Bounty Hunter)
CalTack (embedded ATT&CK website)
SAML (SAML authentication)

Requirements
These requirements are for the computer running the core framework:

Any Linux or MacOS
Python 3.9+ (with Pip3)
Recommended hardware to run on is 8GB+ RAM and 2+ CPUs
Recommended: GoLang 1.17+ to dynamically compile GoLang-based agents.
NodeJS (v16+ recommended for v5 VueJS UI)

Installation
Concise installation steps:
git clone https://github.com/mitre/caldera.git --recursive
cd caldera
pip3 install -r requirements.txt
python3 server.py --insecure --build
Full steps:
Start by cloning this repository recursively, passing the desired version/release in x.x.x format. This will pull in all available plugins.
git clone https://github.com/mitre/caldera.git --recursive --tag x.x.x
Next, install the PIP requirements:
pip3 install -r requirements.txt
Super-power your Caldera server installation! Install GoLang (1.19+)
Finally, start the server.
python3 server.py --insecure --build
The --build flag automatically installs any VueJS UI dependencies, bundles the UI into a dist directory and is served by the Caldera server. You will only have to use the --build flag again if you add any plugins or make any changes to the UI. Once started, log into http://localhost:8888 using the default credentials red/admin. Then go into Plugins -> Training and complete the capture-the-flag style training course to learn how to use Caldera.
If you prefer to not use the new VueJS UI, revert to Caldera v4.2.0. Correspondingly, do not use the --build flag for earlier versions as not required.
Additionally, please note security recommendations for deploying Caldera.
Docker Installation
Local build:
git clone https://github.com/mitre/caldera.git --recursive
cd caldera
docker build --build-arg VARIANT=full -t caldera .
docker run -it -p 8888:8888 caldera
Adjust the port forwarding (-p) and build args (--build-arg) as desired to make ports accessible or change the Caldera variant. The ports that you expose depend on which contacts you plan on using (see Dockerfile and docker-compose.yml for reference).
Pre-Built Image (from GitHub Container Registry):
docker run -p 8888:8888 ghcr.io/mitre/caldera:latest
This container may be slightly outdated, we recommend building the container yourself.
To gracefully terminate your docker container, do the following:
# Find the container ID for your docker container running Caldera
docker ps

# Stop the container
docker stop <container ID>
There are two variants available, full and slim. The slim variant doesn't include files necessary for the emu and atomic plugins, which will be downloaded on-demand if the plugins are ever enabled. The full variant is suitable for operation in environments without an internet connection. Slim images on GHCR are prefixed with "slim".
Docker Container Notes

The Caldera container will automatically generate keys/usernames/password on first start.
If you wish to override the default configuration or avoid automatically generated keys/passwords, consider bind-mounting your own configuration file with the -v <your_path>/conf.yml:/usr/src/app/conf/local.yml flag.
Data stored by Caldera is ephemeral by default. If you wish to make it persistent, use docker volumes and/or bind mounts (-v <path_to_your_data_or_volume_name>:/usr/src/app/data/). Ensure that the directory structure is the same as in the data/ directory on GitHub, as Caldera will refuse to create these sub-directories if they are missing. Lastly, make sure that the configuration file is also made persistent to prevent issues with encryption keys.
The builder plugin will not work within Docker.
If you wish to modify data used by the atomic plugin, clone the Atomic Red Team repository outside the container, apply your modifications and bind-mount it (-v) to /usr/src/app/plugins/atomic/data/atomic-red-team within the container.
If you wish to modify data used by emu, clone the adversary_emulation_library repository locally and bind-mount it (-v) to /usr/src/app/plugins/emu/data/adversary-emulation-plans.

Additionally, please note security recommendations for deploying Caldera.
User Interface Development
If you'll be developing the UI, there are a few more additional installation steps.
Requirements

NodeJS (v16+ recommended)

Setup

Add the Magma submodule if you haven't already: git submodule add https://github.com/mitre/magma
Install NodeJS dependencies: cd plugins/magma && npm install && cd ..
Start the Caldera server with an additional flag: python3 server.py --uidev localhost

Your Caldera server is available at http://localhost:8888 as usual, but there will now be a hot-reloading development server for the VueJS front-end available at http://localhost:3000. Both logs from the server and the front-end will display in the terminal you launched the server from.
Security
The Caldera team highly reccommends standing up the Caldera server on a secure environment/network, and not exposing it to the internet. The Caldera server does not have a hardened and thoroughly pentested web application interface, but only basic authentication and security features. Both MITRE and MITRE's US Government sponsors nearly exclusively only use Caldera on secure environments and do not rely on Caldera's own security protocols for proper cyber security.
Vulnerability Disclosures
Refer to our Vulnerability Disclosure Documentation for submitting bugs.
Recent Vulnerability Disclosures
🚨Security Notice🚨: (17 Feb 2025 10:00 EST) Please pull v5.1.0+ for a recent security patch for CVE-2025-27364. Please update your Caldera instance, especially if you host Caldera on a publicly accessible network. Vulnerability walkthrough.
Contributing
Refer to our contributor documentation.
Licensing
To discuss licensing opportunities, please reach out to caldera@mitre.org or directly to MITRE's Technology Transfer Office.
Caldera Benefactor Program
If you are interested in partnering to support, sustain, and evolve MITRE Caldera™'s open source capabilities, please contact us at caldera@mitre.org.

==========

Repository: html5-boilerplate
URL: https://github.com/kavspvt2803/html5-boilerplate
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
HTML5 Boilerplate




HTML5 Boilerplate is a professional front-end template for building
fast, robust, and adaptable web apps or sites.
This project is the product of over 10 years of iterative development and
community knowledge. It does not impose a specific development
philosophy or framework, so you're free to architect your code in the
way that you want.

Homepage
Source Code

About This Repository
This repository is where HTML5-Boilerplate is authored. Some of the tools,
files and processes that you see here are solely for the production of
HTML5 Boilerplate and are not part of HTML5 Boilerplate. For one example, the
gulpfile.mjs
script is used to build the project. It's not part of the project itself.
The project we publish is represented by the contents of the /dist/
folder. Everything else in this repository is used to author the project.
Think of it this way, in the same way that you don't clone vuejs/core
to create a Vue.js app, you don't need to clone this repository to start a new
site or app based on HTML5 Boilerplate.
So, if you're looking for a quick start template to build a website or
application, look at the options in the
Quick Start section of this document.
If you want to help us improve HTML5 Boilerplate then you can start with the documentation here, which includes steps to clone this repo in order to get it set up for development.
Quick Start
Choose one of the following options:


Using the create-html5-boilerplate
script, instantly fetch the latest npm published package (or any version
available on npm) with npx, npm init or yarn create without having to
install any dependencies. Running the following npx command installs the
latest version into a folder called new-site
npx create-html5-boilerplate new-site
cd new-site
npm install
npm run start


Using our new Template Repository
create a new GitHub repository based on the latest code from the main branch of HTML5
Boilerplate.


Install with npm: npm install html5-boilerplate
or yarn: yarn add html5-boilerplate. The resulting
node_modules/html5-boilerplate/dist folder represents the latest version of
the project for end users. Depending on what you want to use and how you want
to use it, you may have to copy and paste the contents of that folder into
your project directory.


Download the latest stable release from
here. This zip file is a
snapshot of the dist folder. On Windows, Mac and from the file manager on
Linux unzipping this folder will output to a folder named something like
html5-boilerplate_v9.0.0. From the command-line, you will need to create a
folder and unzip the contents into that folder.
mkdir html5-boilerplate
unzip html5-boilerplate*.zip -d html5-boilerplate


Features

A finely-tuned starter template: Reap the benefits of 10 years of analysis,
research and experimentation by over 200 contributors.
Designed with progressive enhancement in mind.
Includes:

Placeholder Open Graph elements and attributes.
An example package.json file with WebPack commands
built in to jumpstart application development.
Placeholder CSS Media Queries.
Useful CSS helper classes.
Default print styles, performance optimized.
"Delete-key friendly." Easy to strip out parts you don't need.
Extensive documentation.



Browser Support
HTML5-Boilerplate supports the latest, stable releases of all major browsers.
Check the default configuration from Browserslist
for more details on browsers and versions covered.
Documentation
Take a look at the documentation table of contents. This
documentation is bundled with the project which makes it available for offline
reading and provides a useful starting point for any documentation you want to
write about your project.
Contributing
Hundreds of developers have helped to make the HTML5 Boilerplate. Anyone is
welcome to contribute. However, if you decide to get
involved, please take a moment to review the guidelines:

Bug reports
Feature requests
Pull requests

License
The code is available under the MIT license.

==========

Repository: gnuradio
URL: https://github.com/kavspvt2803/gnuradio
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
GNU Radio













GNU Radio is a free & open-source signal processing runtime and signal processing
software development toolkit. Originally developed for use with software-defined
radios and for simulating wireless communications, it's robust capabilities have
led to adoption in hobbyist, academic, and commercial environments. GNU Radio has
found use in software-defined radio, digital communications, nuclear physics, high-
energy particle physics, astrophysics, radio astronomy and more!
Helpful Links

GNU Radio Website
GNU Radio Wiki
Github issue tracker for bug reports and feature requests
View the GNU Radio Mailing List Archive
Subscribe to the GNU Radio Mailing List
GNU Radio Chatroom on Matrix
Contributors and Affiliated Organizations

How to Run the QT Version of GNU Radio Companion
    # Ensure PyQt5 and other dependencies are installed
    pip install QDarkStyle qtpy
    
    # If you want to run tests
    pip install pytest-qt pyautogui

    # Build and install as usual (described below) and then run
    gnuradio-companion --qt
GNU Radio 4.0
The next major release of GNU Radio, GNU Radio 4.0, is currently under active development. The effort is being spearheaded by FAIR (The Facility for Anti-Proton and Ion Research), part of GSI in Germany. Development of that version is currently taking place under their GitHub organization, here.
How to Install GNU Radio
Prebuilt Binaries
The recommended way to install GNU Radio on most platforms is using available binary package distributions.
The following command is for Debian, Ubuntu, and derivatives. Consult your distribution information to obtain the version of GNU Radio which is included.
    sudo apt install gnuradio

For other operating systems and versions, see Quick Start
Ubuntu PPA Installation
For Ubuntu, the latest builds (both released and pulled from master branch) are maintained as PPAs (Personal Package Archives) on launchpad.net. Be sure to uninstall any previously installed versions of gnuradio first. See UnInstall GR.
Other Installation Methods
Platform-specific guides and Cross-platform guides are described in Other Installation Methods.
From Source
Complete instructions for building GNU Radio from source code are detailed in
Installing From Source.
PyBOMBS
We are no longer recommending PyBOMBS to install modern versions of GNU Radio.
Supporting GNU Radio
If you find GNU Radio useful and would like to support its development, you can make a donation. Your contributions help ensure ongoing improvements and maintenance. Thank you for your support!
Legal Matters
Some files have been changed many times throughout the years. Copyright
notices at the top of source files list which years changes have been
made. For some files, changes have occurred in many consecutive years.
These files may often have the format of a year range (e.g., "2006 - 2011"),
which indicates that these files have had copyrightable changes made
during each year in the range, inclusive.

==========

Repository: Basic_Network_Scanner
URL: https://github.com/kavspvt2803/Basic_Network_Scanner
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Network Scanner
A command-line tool for analyzing hosts on a network using various scanning techniques including ICMP echo requests, TCP port scanning, and ARP scanning. This tool is designed for network administrators and security professionals to assess network configurations and identify potential vulnerabilities.
Features

ICMP Echo Request (Ping) Scanning: Quickly check the availability of hosts.
TCP Port Scanning: Identify open ports and services running on a host.
ARP Network Scanning: Discover devices on a local network.
Configurable Scan Types and Timeouts: Customize scans to suit your needs.
Support for Port Ranges: Scan specific ports or entire ranges.
Detailed Scan Results Output: Get comprehensive information about each scan.

Requirements

Python 3.6 or higher: Ensure you have the correct version of Python installed.
Scapy 2.5.0 or higher: Required for packet crafting and network interactions.
Root/Administrator Privileges: Necessary for raw socket operations.

Installation

Clone the Repository:

git clone https://github.com/yourusername/network-scanner.git
cd network-scanner

Install Dependencies:

pip install -r requirements.txt
Usage
Basic usage:
python src/main.py <target> [options]
Examples

Scan a Single Host with All Scan Types:

python src/main.py 192.168.1.1

Perform Only ICMP Scan:

python src/main.py 192.168.1.1 -t icmp

Scan Specific Ports:

python src/main.py 192.168.1.1 -p 80,443,8080

Scan a Port Range:

python src/main.py 192.168.1.1 -p 1-1000

Scan a Network Subnet:

python src/main.py 192.168.1.0/24 -t arp
Command Line Options

target: Target IP address or network (required)
-t, --type: Type of scan to perform (choices: all, icmp, tcp, arp; default: all)
-p, --ports: Ports to scan (e.g., 80,443 or 1-1000)
-T, --timeout: Timeout in seconds for each scan (default: 2)

Project Structure
network-scanner/
├── src/
│   ├── main.py         # Command-line interface
│   └── scanner.py      # Core scanning functionality
├── tests/              # Test files
├── docs/               # Documentation
├── requirements.txt    # Project dependencies
└── README.md           # Project documentation

Security Considerations

Intrusive Nature: This tool performs network scanning which may be considered intrusive.
Permission: Use only on networks you have permission to scan.
Network Restrictions: Some networks may block ICMP or ARP requests.
Privileges: Running the tool requires elevated privileges.

License
This project is licensed under the MIT License - see the LICENSE file for details.

==========

Repository: maigret
URL: https://github.com/kavspvt2803/maigret
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
Maigret






















The Commissioner Jules Maigret is a fictional French police detective, created by Georges Simenon. His investigation method is based on understanding the personality of different people and their interactions.
👉👉👉 Online Telegram bot
About
Maigret collects a dossier on a person by username only, checking for accounts on a huge number of sites and gathering all the available information from web pages. No API keys are required. Maigret is an easy-to-use and powerful fork of Sherlock.
Currently supports more than 3000 sites (full list), search is launched against 500 popular sites in descending order of popularity by default. Also supported checking Tor sites, I2P sites, and domains (via DNS resolving).
Powered By Maigret
These are professional tools for social media content analysis and OSINT investigations that use Maigret (banners are clickable).



Main features

Profile page parsing, extraction of personal info, links to other profiles, etc.
Recursive search by new usernames and other IDs found
Search by tags (site categories, countries)
Censorship and captcha detection
Requests retries

See the full description of Maigret features in the documentation.
Installation
‼️ Maigret is available online via official Telegram bot. Consider using it if you don't want to install anything.
Windows
Standalone EXE-binaries for Windows are located in Releases section of GitHub repository.
Video guide on how to run it: https://youtu.be/qIgwTZOmMmM.
Installation in Cloud Shells
You can launch Maigret using cloud shells and Jupyter notebooks. Press one of the buttons below and follow the instructions to launch it in your browser.




Local installation
Maigret can be installed using pip, Docker, or simply can be launched from the cloned repo.
NOTE: Python 3.10 or higher and pip is required, Python 3.11 is recommended.
# install from pypi
pip3 install maigret

# usage
maigret username
Cloning a repository
# or clone and install manually
git clone https://github.com/soxoj/maigret && cd maigret

# build and install
pip3 install .

# usage
maigret username
Docker
# official image
docker pull soxoj/maigret

# usage
docker run -v /mydir:/app/reports soxoj/maigret:latest username --html

# manual build
docker build -t maigret .
Usage examples
# make HTML, PDF, and Xmind8 reports
maigret user --html
maigret user --pdf
maigret user --xmind #Output not compatible with xmind 2022+

# search on sites marked with tags photo & dating
maigret user --tags photo,dating

# search on sites marked with tag us
maigret user --tags us

# search for three usernames on all available sites
maigret user1 user2 user3 -a
Use maigret --help to get full options description. Also options are documented.
Web interface
You can run Maigret with a web interface, where you can view the graph with results and download reports of all formats on a single page.

Web Interface Screenshots



Instructions:

Run Maigret with the --web flag and specify the port number.

maigret --web 5000


Open http://127.0.0.1:5000 in your browser and enter one or more usernames to make a search.


Wait a bit for the search to complete and view the graph with results, the table with all accounts found, and download reports of all formats.


Contributing
Maigret has open-source code, so you may contribute your own sites by adding them to data.json file, or bring changes to it's code!
For more information about development and contribution, please read the development documentation.
Demo with page parsing and recursive username search
Video (asciinema)



Reports
PDF report, HTML report


Full console output
Disclaimer
This tool is intended for educational and lawful purposes only. The developers do not endorse or encourage any illegal activities or misuse of this tool. Regulations regarding the collection and use of personal data vary by country and region, including but not limited to GDPR in the EU, CCPA in the USA, and similar laws worldwide.
It is your sole responsibility to ensure that your use of this tool complies with all applicable laws and regulations in your jurisdiction. Any illegal use of this tool is strictly prohibited, and you are fully accountable for your actions.
The authors and developers of this tool bear no responsibility for any misuse or unlawful activities conducted by its users.
Feedback
If you have any questions, suggestions, or feedback, please feel free to open an issue, create a GitHub discussion, or contact the author directly via Telegram.
SOWEL classification
This tool uses the following OSINT techniques:

SOTL-2.2. Search For Accounts On Other Platforms
SOTL-6.1. Check Logins Reuse To Find Another Account
SOTL-6.2. Check Nicknames Reuse To Find Another Account

License
MIT © Maigret
MIT © Sherlock Project
Original Creator of Sherlock Project - Siddharth Dushantha

==========

Repository: ChatterBot
URL: https://github.com/kavspvt2803/ChatterBot
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
ChatterBot
ChatterBot is a machine-learning based conversational dialog engine built in
Python which makes it possible to generate responses based on collections of
known conversations. The language independent design of ChatterBot allows it
to be trained to speak any language.





An example of typical input would be something like this:

user: Good morning! How are you doing?
bot:  I am doing very well, thank you for asking.
user: You're welcome.
bot: Do you like hats?

How it works
An untrained instance of ChatterBot starts off with no knowledge of how to communicate. Each time a user enters a statement, the library saves the text that they entered and the text that the statement was in response to. As ChatterBot receives more input the number of responses that it can reply and the accuracy of each response in relation to the input statement increase. The program selects the closest matching response by searching for the closest matching known statement that matches the input, it then returns the most likely response to that statement based on how frequently each response is issued by the people the bot communicates with.
Documentation
View the documentation
for ChatterBot.
Installation
This package can be installed from PyPi by running:
pip install chatterbot
Basic Usage
from chatterbot import ChatBot
from chatterbot.trainers import ChatterBotCorpusTrainer

chatbot = ChatBot('Ron Obvious')

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train the chatbot based on the english corpus
trainer.train("chatterbot.corpus.english")

# Get a response to an input statement
chatbot.get_response("Hello, how are you today?")
Training data
ChatterBot comes with a data utility module that can be used to train chat bots.
At the moment there is training data for over a dozen languages in this module.
Contributions of additional training data or training data
in other languages would be greatly appreciated. Take a look at the data files
in the chatterbot-corpus
package if you are interested in contributing.
from chatterbot.trainers import ChatterBotCorpusTrainer

# Create a new trainer for the chatbot
trainer = ChatterBotCorpusTrainer(chatbot)

# Train based on the english corpus
trainer.train("chatterbot.corpus.english")

# Train based on english greetings corpus
trainer.train("chatterbot.corpus.english.greetings")

# Train based on the english conversations corpus
trainer.train("chatterbot.corpus.english.conversations")
Corpus contributions are welcome! Please make a pull request.
Examples
For examples, see the examples
section of the documentation.
History
See release notes for changes https://github.com/gunthercox/ChatterBot/releases
Development pattern for contributors

Create a fork of
the main ChatterBot repository on GitHub.
Make your changes in a branch named something different from master, e.g. create
a new branch my-pull-request.
Create a pull request.
Please follow the Python style guide for PEP-8.
Use the projects built-in automated testing.
to help make sure that your contribution is free from errors.

License
ChatterBot is licensed under the BSD 3-clause license.

==========

Repository: gun
URL: https://github.com/kavspvt2803/gun
Commits: 0
Branches: 0
Releases: 0
Contributors: 0

README Content:
GUN is an ecosystem of tools that let you build community run and encrypted applications - like an Open Source Firebase or a Decentralized Dropbox.
The Internet Archive and 100s of other apps run GUN in-production.

Multiplayer by default with realtime p2p state synchronization!
Graph data lets you use key/value, tables, documents, videos, & more!
Local-first, offline, and decentralized with end-to-end encryption.

Decentralized alternatives to Zoom, Reddit, Instagram, Slack, YouTube, Stripe, Wikipedia, Facebook Horizon and more have already pushed terabytes of daily P2P traffic on GUN. We are a friendly community creating a free fun future for freedom:








Quickstart
GUN is super easy to get started with:

Try the interactive tutorial in the browser (5min ~ average developer).
Or npm install gun and run the examples with cd node_modules/gun && npm start (5min ~ average developer).


Note: If you don't have node or npm, read this first.
If the npm command line didn't work, you may need to mkdir node_modules first or use sudo.


An online demo of the examples are available here: http://try.axe.eco/
Or write a quick app: (try now in a playground)

<script src="https://cdn.jsdelivr.net/npm/gun/gun.js"></script>
<script>
// import GUN from 'gun'; // in ESM
// GUN = require('gun'); // in NodeJS
// GUN = require('gun/gun'); // in React
gun = GUN();

gun.get('mark').put({
  name: "Mark",
  email: "mark@gun.eco",
});

gun.get('mark').on((data, key) => {
  console.log("realtime updates:", data);
});

setInterval(() => { gun.get('mark').get('live').put(Math.random()) }, 9);
</script>

Or try something mind blowing, like saving circular references to a table of documents! (play)

cat = {name: "Fluffy", species: "kitty"};
mark = {boss: cat};
cat.slave = mark;

// partial updates merge with existing data!
gun.get('mark').put(mark);

// access the data as if it is a document.
gun.get('mark').get('boss').get('name').once(function(data, key){
  // `once` grabs the data once, no subscriptions.
  console.log("Mark's boss is", data);
});

// traverse a graph of circular references!
gun.get('mark').get('boss').get('slave').once(function(data, key){
  console.log("Mark is the cat's slave!", data);
});

// add both of them to a table!
gun.get('list').set(gun.get('mark').get('boss'));
gun.get('list').set(gun.get('mark'));

// grab each item once from the table, continuously:
gun.get('list').map().once(function(data, key){
  console.log("Item:", data);
});

// live update the table!
gun.get('list').set({type: "cucumber", goal: "jumping cat"});
Want to keep building more? Jump to THE DOCUMENTATION!
About
First & foremost, GUN is a community of the nicest and most helpful people out there. So I want to invite you to come tell us about what you are working on & wanting to build (new or old school alike! Just be nice as well.) and ask us your questions directly. :)
Watch the 100 second intro!
The GUN ecosystem stack is a collection of independent and modular tools covering everything from CRDT conflict resolution, cryptographic security & encryption, radix storage serialization, mesh networking & routing algorithms, to distributed systems correctness & load testing, CPU scheduled JSON parser to prevent UI lag, and more!


On that note, let's get some official shout outs covered first:
Support

Thanks to:







           

  


Robert Heessels,
Lorenzo Mangani,
NLnet Foundation,
Sam Liu,
Daniel Dombrowsky,
Vincent Woo,
AJ ONeal,
Bill Ottman,
Mike Lange,
Sean Matheson,
Alan Mimms,
Dário Freire,
John Williamson,
Robin Bron,
Elie Makhoul,
Mike Staub,
Bradley Matusiak,
Jeff Cook,
Nico,
Aaron Artille,
Tim Robinson,
Fabian Stamm,
Mike Staub,
Hunter Owens,
Jacob Millner,
Gerrit Balindt,
Gabriel Lemon,
Murage Martin,
Jason Stallings


Join others in sponsoring code: https://www.patreon.com/gunDB !
Ask questions: http://stackoverflow.com/questions/tagged/gun ?
Found a bug? Report at: https://github.com/amark/gun/issues ;
Need help? Chat with us: http://chat.gun.eco .

History
GUN was created by Mark Nadal in 2014 after he had spent 4 years trying to get his collaborative web app to scale up with traditional databases.
 After he realized Master-Slave database architecture causes one big bottleneck, he (as a complete newbie outsider) naively decided to question the status quo and shake things up with controversial, heretical, and contrarian experiments:
The NoDB - no master, no servers, no "single source of truth", not built with a real programming language or real hardware, no DevOps, no locking, not just SQL or NoSQL but both (all - graphs, documents, tables, key/value).
The goal was to build a P2P database that could survive living inside any browser, and could correctly sync data between any device after assuming any offline-first activity.

Technically, GUN is a graph synchronization protocol with a lightweight embedded engine, capable of doing 20M+ API ops/sec in just ~9KB gzipped size.
Documentation


API reference
Tutorials
Examples


GraphQL
Electron
React & Native


Vue
Svelte
Webcomponents


CAP Theorem Tradeoffs
How Data Sync Works
How GUN is Built


Crypto Auth
Modules
Roadmap


This would not be possible without community contributors, big shout out to:
ajmeyghani (Learn GUN Basics with Diagrams); anywhichway (Block Storage); beebase (Quasar); BrockAtkinson (brunch config); Brysgo (GraphQL); d3x0r (SQLite); forrestjt (file.js); hillct (Docker); JosePedroDias (graph visualizer); JuniperChicago (cycle.js bindings); jveres (todoMVC); kristianmandrup (edge); Lightnet (Awesome Vue User Examples & User Kitchen Sink Playground); lmangani (Cytoscape Visualizer, Cassandra, Fastify, LetsEncrypt); mhelander (SEA); omarzion (Sticky Note App); PsychoLlama (LevelDB); RangerMauve (schema); robertheessels (gun-p2p-auth); rogowski (AXE); sbeleidy; sbiaudet (C# Port); Sean Matheson (Observable/RxJS/Most.js bindings); Shadyzpop (React Native example); sjones6 (Flint); RIP Stefdv (Polymer/web components); zrrrzzt (JWT Auth); xmonader (Python Port);
I am missing many others, apologies, will be adding them soon! This list is infinitely old & way out of date, if you want to be listed in it please make a PR! :)
Testing
You will need to npm install -g mocha first. Then in the gun root folder run npm test. Tests will trigger persistent writes to the DB, so subsequent runs of the test will fail. You must clear the DB before running the tests again. This can be done by running rm -rf *data* command in the project directory.
Shims

These are only needed for NodeJS & React Native, they shim the native Browser WebCrypto API.

If you want to use SEA for User auth and security, you will need to install:
npm install @peculiar/webcrypto --save
Please see our React Native docs for installation instructions!
Then you can require SEA without an error:
GUN = require('gun/gun');
SEA = require('gun/sea');
Deploy

Note: The default examples that get auto-deployed on npm start CDN-ify all GUN files, modules, & storage.


Note: Moving forward, AXE will start to automatically cluster your peer into a shared DHT. You may want to disable this to run an isolated network.


Note: When deploying a web application using GUN on a cloud provider, you may have to set CI=false in your .env. This prevents GUN-specific warnings from being treated as errors when deploying your app. You may also resolve this by modifying your webpack config to not try to build the GUN dependencies.

To quickly spin up a GUN relay peer for your development team, utilize Heroku, Docker, or any others listed below. Or some variant thereof Dokku, K8s, etc. ! Or use all of them so your relays are decentralized too!
Linux
SSH into the home directory of a clean OS install with sudo ability. Set any environment variables you need (see below), then do:
curl -o- https://raw.githubusercontent.com/amark/gun/master/examples/install.sh | bash

Read install.sh first!
If curl is not found, copy&paste the contents of install.sh into your ssh.

You can now safely CTRL+A+D to escape without stopping the peer. To stop everything killall screen or killall node.
Environment variables may need to be set like export HTTPS_CERT=~/cert.pem HTTPS_KEY=~/key.pem PORT=443. You can also look at a sample nginx config. For production deployments, you probably will want to use something like pm2 or better to keep the peer alive after machine reboots.
Dome
Deploy GUN in one-click with Dome and receive a free trial:

Heroku


Heroku deletes your data every 15 minutes, one way to fix this is by adding cheap storage.

Or:
git clone https://github.com/amark/gun.git
cd gun
heroku create
git push -f heroku HEAD:master
Then visit the URL in the output of the 'heroku create' step, in a browser. Make sure to set any environment config vars in the settings tab.
Zeet.co

Then visit the URL in the output of the 'now --npm' step, in your browser.
Docker

Warning: Docker image is community contributed and may be old with missing security updates, please check version numbers to compare.

   
Pull from the Docker Hub . Or:
docker run -p 8765:8765 gundb/gun
Or build the Docker image locally:
git clone https://github.com/amark/gun.git
cd gun
docker build -t myrepo/gundb:v1 .
docker run -p 8765:8765 myrepo/gundb:v1
Or, if you prefer your Docker image with metadata labels (Linux/Mac only):
npm run docker
docker run -p 8765:8765 username/gun:git
Then visit http://localhost:8765 in your browser.
License
Designed with ♥ by Mark Nadal, the GUN team, and many amazing contributors.
Openly licensed under Zlib / MIT / Apache 2.0.

YouTube . Twitter

==========

